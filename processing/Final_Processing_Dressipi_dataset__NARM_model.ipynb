{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Import libraries"
      ],
      "metadata": {
        "id": "kMmoViy8enZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "KGO9k1hUel1w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data (session_id, item_id, date, Datetime, Timestamp)\n",
        "url01 = 'https://raw.githubusercontent.com/anhphuongnguyenquynh/session-based-recsys-fashion/main/dataset_filtered/train_session01_seq.csv'"
      ],
      "metadata": {
        "id": "Lbk2yCxCrAsN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/anhphuongnguyenquynh/session-based-recsys-fashion/main/dressipi_recsys2022_datasets.zip'\n",
        "!wget $url\n",
        "!unzip dressipi_recsys2022_datasets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6BUjsGRfzYP",
        "outputId": "688b556c-3af4-45fc-f305-8a1509b1b5b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-07 09:35:44--  https://raw.githubusercontent.com/anhphuongnguyenquynh/session-based-recsys-fashion/main/dressipi_recsys2022_datasets.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79384785 (76M) [application/zip]\n",
            "Saving to: ‘dressipi_recsys2022_datasets.zip’\n",
            "\n",
            "dressipi_recsys2022 100%[===================>]  75.71M   311MB/s    in 0.2s    \n",
            "\n",
            "2024-06-07 09:35:44 (311 MB/s) - ‘dressipi_recsys2022_datasets.zip’ saved [79384785/79384785]\n",
            "\n",
            "Archive:  dressipi_recsys2022_datasets.zip\n",
            "   creating: dressipi_recsys2022_dataset/\n",
            "  inflating: dressipi_recsys2022_dataset/README.txt  \n",
            "  inflating: dressipi_recsys2022_dataset/candidate_items.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/item_features.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_final_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_final_sessions.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_full_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_full_sessions.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_leaderboard_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_leaderboard_sessions.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/train_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/train_sessions.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test split"
      ],
      "metadata": {
        "id": "sEG2LwNgWLAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset01 = pd.read_csv(url01, index_col = 0, parse_dates=[\"date\"])\n",
        "#dataset01 = pd.read_csv('dressipi_recsys2022_dataset/train_sessions.csv')\n",
        "dataset01 = dataset01.dropna()\n",
        "dataset01 = dataset01.reset_index()\n",
        "#fraction\n",
        "#dataset = dataset01.sample(1)"
      ],
      "metadata": {
        "id": "zsnyFbN7rDlX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset01"
      ],
      "metadata": {
        "id": "xVG7GtbY1TyP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RA4DJebOsis3",
        "outputId": "6740caad-0f28-415b-f148-3848ff89d8d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   session_id  item_id                    date     timestamp  month  weekYear  \\\n",
              "0        18.0   4026.0 2020-08-26 19:15:47.232  1.598469e+09    8.0      35.0   \n",
              "1        19.0  20033.0 2020-11-02 16:42:02.287  1.604335e+09   11.0      45.0   \n",
              "2        19.0   6704.0 2020-11-02 16:47:36.119  1.604336e+09   11.0      45.0   \n",
              "3        24.0  18476.0 2020-02-26 18:24:32.770  1.582741e+09    2.0       9.0   \n",
              "4        36.0  26536.0 2020-06-21 10:29:51.337  1.592735e+09    6.0      25.0   \n",
              "\n",
              "   season  duration  \n",
              "0     2.0     753.0  \n",
              "1     3.0       1.0  \n",
              "2     3.0       1.0  \n",
              "3     4.0       1.0  \n",
              "4     2.0       1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e0d2205-9f72-4a9b-88cb-26d18e5e96cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>session_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>date</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>month</th>\n",
              "      <th>weekYear</th>\n",
              "      <th>season</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>4026.0</td>\n",
              "      <td>2020-08-26 19:15:47.232</td>\n",
              "      <td>1.598469e+09</td>\n",
              "      <td>8.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>753.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19.0</td>\n",
              "      <td>20033.0</td>\n",
              "      <td>2020-11-02 16:42:02.287</td>\n",
              "      <td>1.604335e+09</td>\n",
              "      <td>11.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.0</td>\n",
              "      <td>6704.0</td>\n",
              "      <td>2020-11-02 16:47:36.119</td>\n",
              "      <td>1.604336e+09</td>\n",
              "      <td>11.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24.0</td>\n",
              "      <td>18476.0</td>\n",
              "      <td>2020-02-26 18:24:32.770</td>\n",
              "      <td>1.582741e+09</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36.0</td>\n",
              "      <td>26536.0</td>\n",
              "      <td>2020-06-21 10:29:51.337</td>\n",
              "      <td>1.592735e+09</td>\n",
              "      <td>6.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e0d2205-9f72-4a9b-88cb-26d18e5e96cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e0d2205-9f72-4a9b-88cb-26d18e5e96cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e0d2205-9f72-4a9b-88cb-26d18e5e96cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6dde66d2-4a73-4203-abfd-38a4cad3e28a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6dde66d2-4a73-4203-abfd-38a4cad3e28a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6dde66d2-4a73-4203-abfd-38a4cad3e28a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter item less than 5 interactions\n",
        "df_item_count = dataset[['item_id', 'session_id']].groupby('item_id').count().sort_values(by = 'session_id', ascending = False)\n",
        "df_item_count.columns = ['CountItemId']\n",
        "df_item_count_5 = df_item_count[df_item_count['CountItemId'] <10]\n",
        "# remove item_id less than 5 interactions\n",
        "dataset = dataset[~dataset['item_id'].isin(list(df_item_count_5.index))]"
      ],
      "metadata": {
        "id": "Ew_udQd9z0bA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get dataset from latest 7 days"
      ],
      "metadata": {
        "id": "k8Fis253iacg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count unique item_id\n",
        "num_items = dataset['item_id'].nunique()\n",
        "print('Number of items: ', num_items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fCJgPyas5S7",
        "outputId": "ba6f31f0-c74c-40e7-c921-a7512f01175f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of items:  11954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Get the maximum date as a string\n",
        "maxdate_str = dataset['date'].max().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Parse the string into a datetime object\n",
        "maxdate = datetime.strptime(maxdate_str, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Calculate the date 7 days before the maximum date\n",
        "mindate7 = maxdate - timedelta(days=7)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "test_data = dataset[dataset['date'] >= mindate7]\n",
        "train_data = dataset[dataset['date'] <= mindate7]"
      ],
      "metadata": {
        "id": "SQsuSFgv217b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train split data test\n",
        "# random_selection = np.random.rand(len(dataset.index)) <= 0.85\n",
        "# train_data = dataset[random_selection]\n",
        "# test_data = dataset[~random_selection]"
      ],
      "metadata": {
        "id": "HcqEGa9qsIqT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dictionary {SessionId:{ItemId1:Timestamp1, ItemId2:Timestamp2, ...}}\n",
        "train_sess = train_data[['session_id', 'item_id', 'timestamp']].groupby('session_id').apply(lambda x: dict(zip(x['item_id'], x['timestamp'])))\n",
        "test_sess = test_data[['session_id', 'item_id', 'timestamp']].groupby('session_id').apply(lambda x: dict(zip(x['item_id'], x['timestamp'])))"
      ],
      "metadata": {
        "id": "qy_IpxDWfLeT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove train_sess less than 2 item_id\n",
        "train_sess = train_sess[train_sess.apply(lambda x: len(x) >= 2)]\n",
        "test_sess = test_sess[test_sess.apply(lambda x: len(x) >= 2)]"
      ],
      "metadata": {
        "id": "ox8_nsEDSo6A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5P_kj2Bphtf",
        "outputId": "5d1ddffb-b367-4fdf-b415-b5fcc16c23da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "session_id\n",
              "19.0         {20033.0: 1604335322.287, 6704.0: 1604335656.119}\n",
              "108.0        {4816.0: 1591462767.995, 13885.0: 1591463520.6...\n",
              "154.0        {21152.0: 1587713021.545, 27613.0: 1587713168....\n",
              "380.0        {14208.0: 1609504400.187, 19112.0: 1609532456....\n",
              "415.0        {27599.0: 1579507402.061, 4040.0: 1579507525.384}\n",
              "                                   ...                        \n",
              "4439895.0    {19454.0: 1605039741.558, 23687.0: 1605039891....\n",
              "4439964.0    {11397.0: 1585037643.053, 10093.0: 1585037838....\n",
              "4439973.0    {1648.0: 1604042533.113, 20829.0: 1604042603.2...\n",
              "4439990.0    {17429.0: 1598096360.419, 22093.0: 1598099595....\n",
              "4440001.0    {25129.0: 1604076172.451, 25273.0: 1604100295....\n",
              "Length: 88895, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing data"
      ],
      "metadata": {
        "id": "oR34_AEOYVDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessDict = {214834865: '1396808691.295000', 214706441: '1396808691.426000', 214820225: '1396808691.422000'}\n",
        "#Function to preprocess create input and output for the sessions\n",
        "def _preprocess_sess_dict(sessDict):\n",
        "    sessDictTime = dict([(v, k) for (k, v) in sessDict.items()])\n",
        "    sessSort = sorted(sessDictTime.items(), reverse = False)\n",
        "    times = [item[0] for item in sessSort]\n",
        "    itemIds = [item[1] for item in sessSort]\n",
        "    inp_seq = []\n",
        "    labels = []\n",
        "    inp_time = []\n",
        "\n",
        "    for i in range(len(sessSort)):\n",
        "        if i >= 1:\n",
        "            inp_seq += [itemIds[:i]]\n",
        "            labels += [itemIds[i]]\n",
        "            inp_time += [times[i]]\n",
        "    return inp_seq, inp_time, labels, itemIds\n",
        "\n",
        "inp_seq, inp_time, labels, itemIds = _preprocess_sess_dict(sessDict)\n",
        "print('input sequences: ', inp_seq)\n",
        "print('input times: ', inp_time)\n",
        "print('targets: ', labels)\n",
        "print('sequence: ', itemIds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPUbfre3aQ3v",
        "outputId": "4f922241-bdbc-4dbf-b7bd-2244c4afbf77"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sequences:  [[214834865], [214834865, 214820225]]\n",
            "input times:  ['1396808691.422000', '1396808691.426000']\n",
            "targets:  [214820225, 214706441]\n",
            "sequence:  [214834865, 214820225, 214706441]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create input and output for sessions\n",
        "def _preprocess_data(data_sess):\n",
        "    inp_seqs = []\n",
        "    inp_times = []\n",
        "    labels = []\n",
        "    sequences = []\n",
        "    sessIds = list(data_sess.index)\n",
        "    for sessId in sessIds:\n",
        "        sessDict = data_sess.loc[sessId]\n",
        "        inp_seq, inp_time, label, sequence = _preprocess_sess_dict(sessDict)\n",
        "        inp_seqs += inp_seq\n",
        "        inp_times += inp_time\n",
        "        labels += label\n",
        "        sequences += sequence\n",
        "    return inp_seqs, inp_times, labels, sequences\n",
        "\n",
        "train_inp_seqs, train_inp_dates, train_labs, train_sequences = _preprocess_data(train_sess)\n",
        "test_inp_seqs, test_inp_dates, test_labs, test_sequences = _preprocess_data(test_sess)\n",
        "\n",
        "train = (train_inp_seqs, train_labs)\n",
        "test = (test_inp_seqs, test_labs)\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "He_tN7Swaivu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ddd37b-91c5-4683-c36c-601d605b0639"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save train/test"
      ],
      "metadata": {
        "id": "547YB5FAapcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def _save_file(filename, obj):\n",
        "  with open(filename, 'wb') as fn:\n",
        "    pickle.dump(obj, fn)\n",
        "\n",
        "# Create folder for save train/test\n",
        "if not os.path.exists('dressipi_data_train0.2'):\n",
        "  os.mkdir('dressipi_data_train0.2')\n",
        "\n",
        "# save train/test\n",
        "_save_file('dressipi_data_train0.2/train02.pkl', train)\n",
        "_save_file('dressipi_data_train0.2/test02.pkl', test)"
      ],
      "metadata": {
        "id": "Kra5Au9Maotg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def _load_file(filename):\n",
        "  with open(filename, 'rb') as fn:\n",
        "    data = pickle.load(fn)\n",
        "  return data\n",
        "\n",
        "# load train and test data from pickle\n",
        "train = _load_file('dressipi_data_train0.2/train02.pkl')\n",
        "test = _load_file('dressipi_data_train0.2/test02.pkl')"
      ],
      "metadata": {
        "id": "xaant_vkrnja"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build vocabulary"
      ],
      "metadata": {
        "id": "3saYIMSOsFMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token default\n",
        "PAD_token = 0  # token padding for short sequence. If nothing, pad_token equal to 0\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.item2index = {}\n",
        "        self.item2count = {}\n",
        "        self.index2item = {PAD_token: \"PAD\"}\n",
        "        self.num_items = 1  #default number items is 1 - for PAD_token\n",
        "\n",
        "    def addSenquence(self, data):\n",
        "        for sequence in data:\n",
        "          for item in sequence:\n",
        "              self.addItem(item)\n",
        "\n",
        "    # add one item into the system\n",
        "    def addItem(self, item):\n",
        "        if item not in self.item2index:\n",
        "            self.item2index[item] = self.num_items\n",
        "            self.item2count[item] = 1\n",
        "            self.index2item[self.num_items] = item\n",
        "            self.num_items += 1\n",
        "        else:\n",
        "            self.item2count[item] += 1\n",
        "\n",
        "    #Remove item exist < min_count\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_items = []\n",
        "\n",
        "        for k, v in self.item2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_items.append(k)\n",
        "\n",
        "        print('keep_items {} / {} = {:.4f}'.format(\n",
        "            len(keep_items), len(self.item2index), len(keep_items) / len(self.item2index)\n",
        "        ))\n",
        "\n",
        "        # Create vocabulary again\n",
        "        self.item2index = {}\n",
        "        self.item2count = {}\n",
        "        self.index2item = {PAD_token: \"PAD\"}\n",
        "        self.num_items = 1\n",
        "\n",
        "        # Add more items into vocabulary\n",
        "        for item in keep_items:\n",
        "            self.addItem(item)\n",
        "\n",
        "    # convert sequence into a list of indices\n",
        "    def _seqItem2seqIndex(self, x):\n",
        "        return [voc.item2index[item] if item in voc.item2index else 0 for item in x]"
      ],
      "metadata": {
        "id": "CbHpMcKysGcU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get all the lists of items in sessionIds\n",
        "from itertools import chain\n",
        "seq_targets = [train[1]] + [test[1]]\n",
        "sessionIds = list(chain.from_iterable(seq_targets))\n",
        "sessionIds = set(sessionIds)\n",
        "print('Number of sessionIds: ', len(sessionIds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrqaqhmzsLVF",
        "outputId": "c2c02816-6ca8-4ebe-9aa7-70f6bd6bc63a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessionIds:  11903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build vocabulary\n",
        "voc = Voc('DictItemId')\n",
        "voc.addSenquence(seq_targets)\n",
        "\n",
        "# Convert and test on one sequence itemIds\n",
        "print('sequence of itemIds: ', train[0][7])\n",
        "print('converted indices: ', voc._seqItem2seqIndex(train[0][7]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abm4-4V2sN1G",
        "outputId": "1b62b3f5-9336-4711-dd76-68bf99503654"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence of itemIds:  [1720.0]\n",
            "converted indices:  [10043]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert train, test into indices\n",
        "train_x_index = [voc._seqItem2seqIndex(seq) for seq in train[0]]\n",
        "test_x_index = [voc._seqItem2seqIndex(seq) for seq in test[0]]\n",
        "train_y_index = voc._seqItem2seqIndex(train[1])\n",
        "test_y_index = voc._seqItem2seqIndex(test[1])\n",
        "train_index = (train_x_index, train_y_index)\n",
        "test_index = (test_x_index, test_y_index)"
      ],
      "metadata": {
        "id": "dPPxgFmVsagN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train/Test/Validation"
      ],
      "metadata": {
        "id": "itMzUJP0s8iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(root='', valid_portion=0.1, maxlen=13, sort_by_len=False, train_set=None, test_set=None):\n",
        "    \"\"\"Load dataset từ root\n",
        "    root: folder dữ liệu train, trong trường hợp train_set, test_set tồn tại thì không sử dụng train_set và test_set\n",
        "    valid_portion: tỷ lệ phân chia dữ liệu validation/train\n",
        "    maxlen: độ dài lớn nhất của sequence\n",
        "    sort_by_len: có sort theo chiều dài các session trước khi chia hay không?\n",
        "    train_set: training dataset\n",
        "    test_set:  test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the dataset\n",
        "    if train_set is None and test_set is None:\n",
        "        path_train_data = os.path.join(root, 'train.pkl')\n",
        "        path_test_data = os.path.join(root, 'test.pkl')\n",
        "        with open(path_train_data, 'rb') as f1:\n",
        "            train_set = pickle.load(f1)\n",
        "\n",
        "        with open(path_test_data, 'rb') as f2:\n",
        "            test_set = pickle.load(f2)\n",
        "\n",
        "    if maxlen:\n",
        "        new_train_set_x = []\n",
        "        new_train_set_y = []\n",
        "        # Lọc dữ liệu sequence đến maxlen\n",
        "        for x, y in zip(train_set[0], train_set[1]):\n",
        "            if len(x) < maxlen:\n",
        "                new_train_set_x.append(x)\n",
        "                new_train_set_y.append(y)\n",
        "            else:\n",
        "                new_train_set_x.append(x[:maxlen])\n",
        "                new_train_set_y.append(y)\n",
        "        train_set = (new_train_set_x, new_train_set_y)\n",
        "        del new_train_set_x, new_train_set_y\n",
        "\n",
        "        new_test_set_x = []\n",
        "        new_test_set_y = []\n",
        "        for xx, yy in zip(test_set[0], test_set[1]):\n",
        "            if len(xx) < maxlen:\n",
        "                new_test_set_x.append(xx)\n",
        "                new_test_set_y.append(yy)\n",
        "            else:\n",
        "                new_test_set_x.append(xx[:maxlen])\n",
        "                new_test_set_y.append(yy)\n",
        "        test_set = (new_test_set_x, new_test_set_y)\n",
        "        del new_test_set_x, new_test_set_y\n",
        "\n",
        "    # phân chia tập train thành train và validation\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    (test_set_x, test_set_y) = test_set\n",
        "\n",
        "    # Trả về indices thứ tự độ dài của mỗi phần tử trong seq\n",
        "    def len_argsort(seq):\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "\n",
        "    # Sắp xếp session theo độ dài tăng dần\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(test_set_x)\n",
        "        test_set_x = [test_set_x[i] for i in sorted_index]\n",
        "        test_set_y = [test_set_y[i] for i in sorted_index]\n",
        "\n",
        "        sorted_index = len_argsort(valid_set_x)\n",
        "        valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
        "        valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
        "\n",
        "    train = (train_set_x, train_set_y)\n",
        "    valid = (valid_set_x, valid_set_y)\n",
        "    test = (test_set_x, test_set_y)\n",
        "    return train, valid, test"
      ],
      "metadata": {
        "id": "vprh-NX3s7jT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader"
      ],
      "metadata": {
        "id": "XqAlGwtitLdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RecSysDataset(Dataset):\n",
        "    \"\"\"define the pytorch Dataset class\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        print('-'*50)\n",
        "        print('Dataset info:')\n",
        "        print('Number of sessions: {}'.format(len(data[0])))\n",
        "        print('-'*50)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        session_items = self.data[0][index]\n",
        "        target_item = self.data[1][index]\n",
        "        return session_items, target_item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])"
      ],
      "metadata": {
        "id": "xyb-k3pAtIx0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm phụ trợ\n",
        "import torch\n",
        "\n",
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    Hàm số này sẽ được sử dụng để pad session về max length\n",
        "    Args:\n",
        "      data: batch truyền vào\n",
        "    return:\n",
        "      batch data đã được pad length có shape maxlen x batch_size\n",
        "    \"\"\"\n",
        "    # Sort batch theo độ dài của input_sequence từ cao xuống thấp\n",
        "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    lens = [len(sess) for sess, label in data]\n",
        "    labels = []\n",
        "    # Padding batch size\n",
        "    padded_sesss = torch.zeros(len(data), max(lens)).long()\n",
        "    for i, (sess, label) in enumerate(data):\n",
        "        padded_sesss[i,:lens[i]] = torch.LongTensor(sess)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Transpose dữ liệu từ batch_size x maxlen --> maxlen x batch_size\n",
        "    padded_sesss = padded_sesss.transpose(0,1)\n",
        "    return padded_sesss, torch.tensor(labels).long(), lens"
      ],
      "metadata": {
        "id": "iIDbw8LitOiG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metric"
      ],
      "metadata": {
        "id": "KsbuMl6ptbEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_recall(indices, targets):\n",
        "    \"\"\"\n",
        "    Tính toán chỉ số recall cho một tập hợp predictions và targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices được dự báo từ mô hình model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "    \"\"\"\n",
        "    # copy targets k lần để trở thành kích thước Bxk\n",
        "    targets = targets.view(-1, 1).expand_as(indices)\n",
        "    # so sánh targets với indices để tìm ra vị trí mà khách hàng sẽ hit.\n",
        "    hits = (targets == indices).to(device)\n",
        "    hits = hits.double()\n",
        "    if targets.size(0) == 0:\n",
        "        return 0\n",
        "    # Đếm số hit\n",
        "    n_hits = torch.sum(hits)\n",
        "    recall = n_hits / targets.size(0)\n",
        "    return recall\n",
        "\n",
        "\n",
        "def get_mrr(indices, targets):\n",
        "    \"\"\"\n",
        "    Tính toán chỉ số MRR cho một tập hợp predictions và targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices được dự báo từ mô hình model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the MRR score\n",
        "    \"\"\"\n",
        "    tmp = targets.view(-1, 1)\n",
        "    targets = tmp.expand_as(indices)\n",
        "    hits = (targets == indices).to(device)\n",
        "    hits = hits.double()\n",
        "    if hits.sum() == 0:\n",
        "      return 0\n",
        "    argsort = []\n",
        "    for i in np.arange(hits.shape[0]):\n",
        "      index_col = torch.where(hits[i, :] == 1)[0]+1\n",
        "      if index_col.shape[0] != 0:\n",
        "        argsort.append(index_col.double())\n",
        "    inv_argsort = [1/item for item in argsort]\n",
        "    mrr = sum(inv_argsort)/hits.shape[0]\n",
        "    return mrr\n",
        "\n",
        "\n",
        "def evaluate(logits, targets, k=20):\n",
        "    \"\"\"\n",
        "    Đánh giá model sử dụng Recall@K, MRR@K scores.\n",
        "    Args:\n",
        "        logits (B,C): torch.LongTensor. giá trị predicted logit cho itemId tiếp theo.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "        mrr (float): the mrr score\n",
        "    \"\"\"\n",
        "    # Tìm ra indices của topk lớn nhất các giá trị dự báo.\n",
        "    _, indices = torch.topk(logits, k, -1)\n",
        "    recall = get_recall(indices, targets)\n",
        "    mrr = get_mrr(indices, targets)\n",
        "    return recall, mrr"
      ],
      "metadata": {
        "id": "CE9JLKXMtdU-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "logits = torch.tensor([[1, 2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)\n",
        "\n",
        "targets = torch.tensor([1, 2, 2]).to(device)\n",
        "\n",
        "recall, mrr = evaluate(logits = logits, targets = targets, k = 2)\n",
        "print(recall, mrr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjaSlyNdtipX",
        "outputId": "b44f31d0-c128-4232-fbd7-42414c94caeb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., dtype=torch.float64) tensor([1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model NARM"
      ],
      "metadata": {
        "id": "bcWAlCNUdV_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class NARM(nn.Module):\n",
        "    def __init__(self, hidden_size, n_items, embedding_dim, n_layers=1, dropout=0.25):\n",
        "        super(NARM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(self.n_items, self.embedding_dim, padding_idx = 0)\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        # set bidirectional = True for bidirectional\n",
        "        self.gru = nn.GRU(input_size = hidden_size, # number of expected feature of input x\n",
        "                          hidden_size = hidden_size, # number of expected feature of hidden state\n",
        "                          num_layers = n_layers, # number of GRU layers\n",
        "                          dropout=(0 if n_layers == 1 else dropout), # dropout probability apply in encoder network\n",
        "                          bidirectional=True # one or two directions.\n",
        "                         )\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.n_layers)\n",
        "        self.a_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.a_2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.v_t = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "        self.ct_dropout = nn.Dropout(0.5)\n",
        "        self.b = nn.Linear(self.embedding_dim, 2 * self.hidden_size, bias=False)\n",
        "        self.sf = nn.Softmax()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        \"\"\"\n",
        "        input_seq: Batch input_sequence. Shape: max_len x batch_size\n",
        "        input_lengths: Batch input lengths. Shape: batch_size\n",
        "        \"\"\"\n",
        "        # Step 1: Convert sequence indexes to embeddings\n",
        "        # shape: (max_length , batch_size , hidden_size)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module. Padding zero when length less than max_length of input_lengths.\n",
        "        # shape: (max_length , batch_size , hidden_size)\n",
        "        packed = pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "        # Step 2: Forward packed through GRU\n",
        "        # outputs is output of final GRU layer\n",
        "        # hidden is concatenate of all hidden states corresponding with each time step.\n",
        "        # outputs shape: (max_length , batch_size , hidden_size x num_directions)\n",
        "        # hidden shape: (n_layers x num_directions , batch_size , hidden_size)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding. Revert of pack_padded_sequence\n",
        "        # outputs shape: (max_length , batch_size , hidden_size x num_directions)\n",
        "        outputs, length = pad_packed_sequence(outputs)\n",
        "\n",
        "        # Step 3: Global Encoder & Local Encoder\n",
        "        # num_directions = 1 -->\n",
        "        # outputs shape:(max_length , batch_size , hidden_size)\n",
        "        # hidden shape: (n_layers , batch_size , hidden_size)\n",
        "        # lấy hidden state tại time step cuối cùng\n",
        "        ht = hidden[-1]\n",
        "        # reshape outputs\n",
        "        outputs = outputs.permute(1, 0, 2) # [batch_size, max_length, hidden_size] #permute to change size of tensor\n",
        "        c_global = ht\n",
        "        # Flatten outputs thành shape: [batch_size, max_length, hidden_size]\n",
        "        gru_output_flatten = outputs.contiguous().view(-1, self.hidden_size)\n",
        "        # Thực hiện một phép chiếu linear projection để tạo các latent variable có shape [batch_size, max_length, hidden_size]\n",
        "        q1 = self.a_1(gru_output_flatten).view(outputs.size())\n",
        "        # Thực hiện một phép chiếu linear projection để tạo các latent variable có shape [batch_size, max_length, hidden_size]\n",
        "        q2 = self.a_2(ht)\n",
        "        # Ma trận mask đánh dấu vị trí khác 0 trên padding sequence.\n",
        "        mask = torch.where(input_seq.permute(1, 0) > 0, torch.tensor([1.], device = self.device), torch.tensor([0.], device = self.device)) # batch_size x max_len\n",
        "        # Điều chỉnh shape\n",
        "        q2_expand = q2.unsqueeze(1).expand_as(q1) # shape [batch_size, max_len, hidden_size]\n",
        "        q2_masked = mask.unsqueeze(2).expand_as(q1) * q2_expand # batch_size x max_len x hidden_size\n",
        "        # Tính trọng số alpha đo lường similarity giữa các hidden state\n",
        "        alpha = self.v_t(torch.sigmoid(q1 + q2_masked).view(-1, self.hidden_size)).view(mask.size()) # batch_size x max_len\n",
        "        alpha_exp = alpha.unsqueeze(2).expand_as(outputs) # batch_size x max_len x hidden_size\n",
        "        # Tính linear combinition của các hidden state\n",
        "        c_local = torch.sum(alpha_exp * outputs, 1) # (batch_size x hidden_size)\n",
        "\n",
        "        # Véc tơ combinition tổng hợp\n",
        "        c_t = torch.cat([c_local, c_global], 1) # batch_size x (2*hidden_size)\n",
        "        c_t = self.ct_dropout(c_t)\n",
        "        # Tính scores\n",
        "\n",
        "        # Step 4: Decoder\n",
        "        # embedding cho toàn bộ các item\n",
        "        item_indices = torch.arange(self.n_items).to(device) # 1 x n_items\n",
        "        item_embs = self.embedding(item_indices) # n_items x embedding_dim\n",
        "        # reduce dimension by bi-linear projection\n",
        "        B = self.b(item_embs).permute(1, 0) # (2*hidden_size) x n_items\n",
        "        scores = torch.matmul(c_t, B) # batch_size x n_items\n",
        "        # scores = self.sf(scores)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "XsuSpwf6dXLQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kiểm tra model NARM\n",
        "# Thử nghiệm model bằng cách giả lập 1 input và thực hiện quá trình feed forward\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "hidden_size = 3\n",
        "n_layers = 7\n",
        "# embedding = nn.Embedding(11000, hidden_size)\n",
        "input_variable = torch.tensor([[  66,  369,   66, 1272],\n",
        "                                [ 567,  183,   28,  616],\n",
        "                                [ 392, 1558, 1143,  175],\n",
        "                                [ 394,   31,   31, 5558],\n",
        "                                [   0,    0,    0,    0]]).to(device)\n",
        "\n",
        "lengths =  torch.tensor([5, 5, 5, 5]).to(device)\n",
        "print('input_seq: \\n', input_variable)\n",
        "print('input_lengths: \\n', lengths)\n",
        "model_test = NARM(hidden_size = hidden_size, n_items  = 100000, embedding_dim = 100, n_layers=1, dropout=0.25).to(device)\n",
        "print('model phrase: \\n', model_test)\n",
        "scores = model_test.forward(input_seq = input_variable, input_lengths = lengths)\n",
        "print('probability distribution: ', scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiCxOA6Jdbqm",
        "outputId": "0cf9d515-16f5-42df-8ad2-66e28a6d6bf1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_seq: \n",
            " tensor([[  66,  369,   66, 1272],\n",
            "        [ 567,  183,   28,  616],\n",
            "        [ 392, 1558, 1143,  175],\n",
            "        [ 394,   31,   31, 5558],\n",
            "        [   0,    0,    0,    0]])\n",
            "input_lengths: \n",
            " tensor([5, 5, 5, 5])\n",
            "model phrase: \n",
            " NARM(\n",
            "  (embedding): Embedding(100000, 100, padding_idx=0)\n",
            "  (gru): GRU(100, 3)\n",
            "  (emb_dropout): Dropout(p=0.25, inplace=False)\n",
            "  (a_1): Linear(in_features=3, out_features=3, bias=False)\n",
            "  (a_2): Linear(in_features=3, out_features=3, bias=False)\n",
            "  (v_t): Linear(in_features=3, out_features=1, bias=False)\n",
            "  (ct_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (b): Linear(in_features=100, out_features=6, bias=False)\n",
            "  (sf): Softmax(dim=None)\n",
            ")\n",
            "probability distribution:  torch.Size([4, 100000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1, 2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)"
      ],
      "metadata": {
        "id": "YE9bClugQ_yP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model_test.forward(input_seq = input_variable, input_lengths = lengths)"
      ],
      "metadata": {
        "id": "imBuVND_RFuT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)\n",
        "print('probability distribution: ', scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9Ti-1ooTrJy",
        "outputId": "ba6bb0bf-7981-4369-c7e2-eacaf5cddb18"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000, -0.2101,  0.1084,  ...,  0.0815, -0.1765, -0.2213],\n",
            "        [ 0.0000, -0.4658,  0.0320,  ...,  0.0689, -0.2474, -0.5909],\n",
            "        [ 0.0000, -0.2487, -0.1121,  ..., -0.2526, -1.2101, -0.7120],\n",
            "        [ 0.0000,  0.8535, -0.0691,  ..., -0.2591,  0.1968,  0.9229]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "probability distribution:  torch.Size([4, 100000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values, indices = torch.topk(scores, 20, -1)"
      ],
      "metadata": {
        "id": "vApg_REgHQH8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(logits = indices, targets = targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAgGk0pCQwdg",
        "outputId": "30e5f5fa-4275-4992-848b-9d4bf5474e3a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.7500, dtype=torch.float64), tensor([0.3107], dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation"
      ],
      "metadata": {
        "id": "Dxql6QPAtoDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, model):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    mrrs = []\n",
        "    with torch.no_grad():\n",
        "        for seq, target, lens in valid_loader:\n",
        "            seq = seq.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs = model(seq, lens)\n",
        "            logits = F.softmax(outputs, dim = 1)\n",
        "            recall, mrr = evaluate(logits, target, k = args['topk'])\n",
        "            recalls.append(recall)\n",
        "            mrrs.append(mrr)\n",
        "    recall_stack = torch.stack(recalls)\n",
        "    mean_recall = torch.mean(torch.stack(recalls))\n",
        "    # mean_mrr = torch.mean(torch.stack(mrrs))\n",
        "    return mean_recall #, mean_mrr"
      ],
      "metadata": {
        "id": "qCNFThZftnt6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training model"
      ],
      "metadata": {
        "id": "7ysBrZ0qtu1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "\n",
        "args = {\n",
        "    'dataset_path':'../input/dressipi_recsys2022_dataset/train_sessions.csv',\n",
        "    'batch_size': 512,\n",
        "    'hidden_size': 100,\n",
        "    'embed_dim': 50,\n",
        "    'epoch': 10,\n",
        "    'lr':0.01,\n",
        "    'lr_dc':0.1,\n",
        "    'lr_dc_step':80,\n",
        "    'test':None,\n",
        "    'topk':20,\n",
        "    'valid_portion':0.1,\n",
        "    'n_items': num_items\n",
        "}\n",
        "\n",
        "here = os.path.dirname(os.getcwd())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def main():\n",
        "    print('Loading data...')\n",
        "    train_data, valid_data, test_data = load_data(train_set=train_index, test_set=test_index)\n",
        "    train_data = RecSysDataset(train_data)\n",
        "    valid_data = RecSysDataset(valid_data)\n",
        "    test_data = RecSysDataset(test_data)\n",
        "    train_loader = DataLoader(train_data, batch_size = args['batch_size'], shuffle = True, collate_fn = collate_fn)\n",
        "    valid_loader = DataLoader(valid_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "    test_loader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "    print('Complete load data!')\n",
        "    n_items = voc.num_items\n",
        "    model = NARM(hidden_size = args['hidden_size'], n_items = args['n_items'], embedding_dim = args['embed_dim'], n_layers=1, dropout=0.25).to(device)\n",
        "    print('complete load model!')\n",
        "\n",
        "    if args['test'] == 'store_true':\n",
        "        ckpt = torch.load('latest_checkpoint.pth.tar')\n",
        "        model.load_state_dict(ckpt['state_dict'])\n",
        "        recall = validate(test_loader, model)*10\n",
        "        print(\"Test: Recall@{}: {:.4f}\".format(args['topk'], recall, args['topk']))\n",
        "        return model\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), args['lr'])\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing = 0.1)\n",
        "    scheduler = StepLR(optimizer, step_size = args['lr_dc_step'], gamma = args['lr_dc'])\n",
        "\n",
        "    print('start training!')\n",
        "    for epoch in tqdm(range(args['epoch'])):\n",
        "        # train for one epoch\n",
        "        trainForEpoch(train_loader, model, optimizer, epoch, args['epoch'], criterion, log_aggr = 100)\n",
        "        scheduler.step(epoch = epoch)\n",
        "        recall = validate(valid_loader, model)*10\n",
        "        print('Epoch {} validation: Recall@{}: {:.4f}\\n'.format(epoch, args['topk'], recall, args['topk']))\n",
        "\n",
        "        # store best loss and save a model checkpoint\n",
        "        ckpt_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(ckpt_dict, 'latest_checkpoint.pth.tar')\n",
        "    return model\n",
        "\n",
        "\n",
        "def trainForEpoch(train_loader, model, optimizer, epoch, num_epochs, criterion, log_aggr=100):\n",
        "    model.train()\n",
        "\n",
        "    sum_epoch_loss = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (seq, target, lens) in enumerate(train_loader):\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seq, lens)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        sum_epoch_loss += loss_val\n",
        "\n",
        "        iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "        if i % log_aggr == 0:\n",
        "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f) (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),\n",
        "                  len(seq) / (time.time() - start)))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff7pHgPZtud_",
        "outputId": "5413d5ed-f1f2-4e75-a444-84cf4e8ef241"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 120219\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 13358\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 4756\n",
            "--------------------------------------------------\n",
            "Complete load data!\n",
            "complete load model!\n",
            "start training!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] epoch 1/10  observation 0/235 batch loss: 10.8305 (avg 10.8305) (16.18 im/s)\n",
            "[TRAIN] epoch 1/10  observation 100/235 batch loss: 9.4049 (avg 9.5171) (23.10 im/s)\n",
            "[TRAIN] epoch 1/10  observation 200/235 batch loss: 9.3059 (avg 9.4503) (18.03 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            " 10%|█         | 1/10 [01:14<11:12, 74.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 validation: Recall@20: 0.2310\n",
            "\n",
            "[TRAIN] epoch 2/10  observation 0/235 batch loss: 9.2242 (avg 9.2242) (15.88 im/s)\n",
            "[TRAIN] epoch 2/10  observation 100/235 batch loss: 9.1462 (avg 9.1971) (28.85 im/s)\n",
            "[TRAIN] epoch 2/10  observation 200/235 batch loss: 9.1577 (avg 9.1789) (19.12 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [02:28<09:52, 74.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 validation: Recall@20: 0.2375\n",
            "\n",
            "[TRAIN] epoch 3/10  observation 0/235 batch loss: 9.0776 (avg 9.0776) (22.05 im/s)\n",
            "[TRAIN] epoch 3/10  observation 100/235 batch loss: 9.0209 (avg 9.0832) (25.21 im/s)\n",
            "[TRAIN] epoch 3/10  observation 200/235 batch loss: 9.1077 (avg 9.0791) (35.82 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [03:41<08:34, 73.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 validation: Recall@20: 0.2570\n",
            "\n",
            "[TRAIN] epoch 4/10  observation 0/235 batch loss: 9.0022 (avg 9.0022) (30.51 im/s)\n",
            "[TRAIN] epoch 4/10  observation 100/235 batch loss: 8.8825 (avg 8.9284) (29.94 im/s)\n",
            "[TRAIN] epoch 4/10  observation 200/235 batch loss: 8.8693 (avg 8.9072) (49.15 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [04:53<07:18, 73.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 validation: Recall@20: 0.3076\n",
            "\n",
            "[TRAIN] epoch 5/10  observation 0/235 batch loss: 8.7339 (avg 8.7339) (33.31 im/s)\n",
            "[TRAIN] epoch 5/10  observation 100/235 batch loss: 8.8024 (avg 8.7443) (30.16 im/s)\n",
            "[TRAIN] epoch 5/10  observation 200/235 batch loss: 8.7304 (avg 8.7520) (34.52 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [06:07<06:07, 73.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 validation: Recall@20: 0.3729\n",
            "\n",
            "[TRAIN] epoch 6/10  observation 0/235 batch loss: 8.5267 (avg 8.5267) (30.61 im/s)\n",
            "[TRAIN] epoch 6/10  observation 100/235 batch loss: 8.6044 (avg 8.5913) (18.90 im/s)\n",
            "[TRAIN] epoch 6/10  observation 200/235 batch loss: 8.5173 (avg 8.5995) (22.49 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [07:17<04:48, 72.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 validation: Recall@20: 0.4364\n",
            "\n",
            "[TRAIN] epoch 7/10  observation 0/235 batch loss: 8.3109 (avg 8.3109) (30.78 im/s)\n",
            "[TRAIN] epoch 7/10  observation 100/235 batch loss: 8.3573 (avg 8.3872) (26.62 im/s)\n",
            "[TRAIN] epoch 7/10  observation 200/235 batch loss: 8.5005 (avg 8.4002) (33.30 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [08:30<03:37, 72.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 validation: Recall@20: 0.5023\n",
            "\n",
            "[TRAIN] epoch 8/10  observation 0/235 batch loss: 8.0666 (avg 8.0666) (28.41 im/s)\n",
            "[TRAIN] epoch 8/10  observation 100/235 batch loss: 8.1493 (avg 8.1525) (52.49 im/s)\n",
            "[TRAIN] epoch 8/10  observation 200/235 batch loss: 8.1704 (avg 8.1959) (33.49 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [09:43<02:25, 72.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 validation: Recall@20: 0.5327\n",
            "\n",
            "[TRAIN] epoch 9/10  observation 0/235 batch loss: 7.9322 (avg 7.9322) (32.21 im/s)\n",
            "[TRAIN] epoch 9/10  observation 100/235 batch loss: 8.0572 (avg 7.9823) (33.51 im/s)\n",
            "[TRAIN] epoch 9/10  observation 200/235 batch loss: 8.1434 (avg 8.0339) (18.10 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [10:54<01:12, 72.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 validation: Recall@20: 0.5812\n",
            "\n",
            "[TRAIN] epoch 10/10  observation 0/235 batch loss: 7.7989 (avg 7.7989) (21.39 im/s)\n",
            "[TRAIN] epoch 10/10  observation 100/235 batch loss: 7.9192 (avg 7.8335) (22.45 im/s)\n",
            "[TRAIN] epoch 10/10  observation 200/235 batch loss: 7.9925 (avg 7.8905) (24.23 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [12:07<00:00, 72.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 validation: Recall@20: 0.5913\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}