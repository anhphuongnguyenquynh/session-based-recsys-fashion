{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Import libraries"
      ],
      "metadata": {
        "id": "kMmoViy8enZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "KGO9k1hUel1w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data (session_id, item_id, date, Datetime, Timestamp)\n",
        "url01 = 'https://raw.githubusercontent.com/anhphuongnguyenquynh/session-based-recsys-fashion/main/dataset_filtered/train_session01_seq.csv'"
      ],
      "metadata": {
        "id": "Lbk2yCxCrAsN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test split"
      ],
      "metadata": {
        "id": "sEG2LwNgWLAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset01 = pd.read_csv(url01, index_col = 0, parse_dates=[\"date\"])\n",
        "dataset01 = dataset01.dropna()\n",
        "dataset01 = dataset01.reset_index()\n",
        "#fraction\n",
        "dataset = dataset01.sample(frac=0.6)"
      ],
      "metadata": {
        "id": "zsnyFbN7rDlX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "xVG7GtbY1TyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RA4DJebOsis3",
        "outputId": "c4377162-d7ac-4021-a67c-e4bcc9b9e78b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        session_id  item_id                    date     timestamp  month  \\\n",
              "439084   3771095.0   9687.0 2020-09-25 21:47:21.686  1.601070e+09    9.0   \n",
              "461152   3961232.0  14858.0 2021-01-23 20:30:51.014  1.611434e+09    1.0   \n",
              "228847   1965170.0  23139.0 2021-01-21 10:18:20.990  1.611224e+09    1.0   \n",
              "44808     385278.0  20952.0 2020-12-11 17:47:03.767  1.607709e+09   12.0   \n",
              "82672     711366.0   6187.0 2021-04-26 20:25:04.436  1.619469e+09    4.0   \n",
              "\n",
              "        weekYear  season  duration  \n",
              "439084      39.0     3.0       1.0  \n",
              "461152       3.0     4.0       1.0  \n",
              "228847       3.0     4.0       1.0  \n",
              "44808       50.0     4.0       1.0  \n",
              "82672       17.0     1.0       1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd76cbd0-0c72-457e-ab8e-f79ed863c145\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>session_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>date</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>month</th>\n",
              "      <th>weekYear</th>\n",
              "      <th>season</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>439084</th>\n",
              "      <td>3771095.0</td>\n",
              "      <td>9687.0</td>\n",
              "      <td>2020-09-25 21:47:21.686</td>\n",
              "      <td>1.601070e+09</td>\n",
              "      <td>9.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461152</th>\n",
              "      <td>3961232.0</td>\n",
              "      <td>14858.0</td>\n",
              "      <td>2021-01-23 20:30:51.014</td>\n",
              "      <td>1.611434e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228847</th>\n",
              "      <td>1965170.0</td>\n",
              "      <td>23139.0</td>\n",
              "      <td>2021-01-21 10:18:20.990</td>\n",
              "      <td>1.611224e+09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44808</th>\n",
              "      <td>385278.0</td>\n",
              "      <td>20952.0</td>\n",
              "      <td>2020-12-11 17:47:03.767</td>\n",
              "      <td>1.607709e+09</td>\n",
              "      <td>12.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82672</th>\n",
              "      <td>711366.0</td>\n",
              "      <td>6187.0</td>\n",
              "      <td>2021-04-26 20:25:04.436</td>\n",
              "      <td>1.619469e+09</td>\n",
              "      <td>4.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd76cbd0-0c72-457e-ab8e-f79ed863c145')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd76cbd0-0c72-457e-ab8e-f79ed863c145 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd76cbd0-0c72-457e-ab8e-f79ed863c145');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-449624da-eaa3-4744-82de-e0dafb903ed7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-449624da-eaa3-4744-82de-e0dafb903ed7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-449624da-eaa3-4744-82de-e0dafb903ed7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thống kê số lượt xuất hiện và lọc ra các ItemId có trên 5 lượt xuất hiện\n",
        "df_item_count = dataset[['item_id', 'session_id']].groupby('item_id').count().sort_values(by = 'session_id', ascending = False)\n",
        "df_item_count.columns = ['CountItemId']\n",
        "df_item_count_5 = df_item_count[df_item_count['CountItemId'] < 5]\n",
        "# Lọc khỏi dataset những ItemId có ít hơn 5 lượt xuất hiện\n",
        "dataset = dataset[~dataset['item_id'].isin(list(df_item_count_5.index))]"
      ],
      "metadata": {
        "id": "Ew_udQd9z0bA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train split data test\n",
        "random_selection = np.random.rand(len(dataset.index)) <= 0.85\n",
        "train_data = dataset[random_selection]\n",
        "test_data = dataset[~random_selection]"
      ],
      "metadata": {
        "id": "HcqEGa9qsIqT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy ra dictionary có dạng {SessionId:{ItemId1:Timestamp1, ItemId2:Timestamp2, ...}}\n",
        "train_sess = train_data[['session_id', 'item_id', 'timestamp']].groupby('session_id').apply(lambda x: dict(zip(x['item_id'], x['timestamp'])))\n",
        "test_sess = test_data[['session_id', 'item_id', 'timestamp']].groupby('session_id').apply(lambda x: dict(zip(x['item_id'], x['timestamp'])))"
      ],
      "metadata": {
        "id": "qy_IpxDWfLeT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5P_kj2Bphtf",
        "outputId": "b2050855-319b-4a20-d1c2-a6dd4da8d65e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "session_id\n",
              "18.0                                  {4026.0: 1598469347.232}\n",
              "108.0                                 {4816.0: 1591462767.995}\n",
              "154.0                                {21152.0: 1587713021.545}\n",
              "170.0                                {20691.0: 1583672243.604}\n",
              "181.0                                 {16417.0: 1585940777.84}\n",
              "                                   ...                        \n",
              "4439942.0                            {17609.0: 1600789639.946}\n",
              "4439964.0                            {11397.0: 1585037643.053}\n",
              "4439973.0                            {21328.0: 1604064770.528}\n",
              "4439990.0    {17429.0: 1598096360.419, 22093.0: 1598099595....\n",
              "4440001.0    {25129.0: 1604076172.451, 25273.0: 1604100295....\n",
              "Length: 206781, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing data"
      ],
      "metadata": {
        "id": "oR34_AEOYVDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessDict = {214834865: '1396808691.295000', 214706441: '1396808691.426000', 214820225: '1396808691.422000'}\n",
        "\n",
        "def _preprocess_sess_dict(sessDict):\n",
        "    sessDictTime = dict([(v, k) for (k, v) in sessDict.items()])\n",
        "    sessSort = sorted(sessDictTime.items(), reverse = False)\n",
        "    times = [item[0] for item in sessSort]\n",
        "    itemIds = [item[1] for item in sessSort]\n",
        "    inp_seq = []\n",
        "    labels = []\n",
        "    inp_time = []\n",
        "\n",
        "    for i in range(len(sessSort)):\n",
        "        if i >= 1:\n",
        "            inp_seq += [itemIds[:i]]\n",
        "            labels += [itemIds[i]]\n",
        "            inp_time += [times[i]]\n",
        "    return inp_seq, inp_time, labels, itemIds\n",
        "\n",
        "inp_seq, inp_time, labels, itemIds = _preprocess_sess_dict(sessDict)\n",
        "print('input sequences: ', inp_seq)\n",
        "print('input times: ', inp_time)\n",
        "print('targets: ', labels)\n",
        "print('sequence: ', itemIds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPUbfre3aQ3v",
        "outputId": "8a4488d0-28fb-4de5-9f48-d2cccec3c109"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sequences:  [[214834865], [214834865, 214820225]]\n",
            "input times:  ['1396808691.422000', '1396808691.426000']\n",
            "targets:  [214820225, 214706441]\n",
            "sequence:  [214834865, 214820225, 214706441]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Khởi tạo chuỗi input và output cho toàn bộ các session\n",
        "def _preprocess_data(data_sess):\n",
        "    inp_seqs = []\n",
        "    inp_times = []\n",
        "    labels = []\n",
        "    sequences = []\n",
        "    sessIds = list(data_sess.index)\n",
        "    for sessId in sessIds:\n",
        "        sessDict = data_sess.loc[sessId]\n",
        "        inp_seq, inp_time, label, sequence = _preprocess_sess_dict(sessDict)\n",
        "        inp_seqs += inp_seq\n",
        "        inp_times += inp_time\n",
        "        labels += label\n",
        "        sequences += sequence\n",
        "    return inp_seqs, inp_times, labels, sequences\n",
        "\n",
        "train_inp_seqs, train_inp_dates, train_labs, train_sequences = _preprocess_data(train_sess)\n",
        "test_inp_seqs, test_inp_dates, test_labs, test_sequences = _preprocess_data(test_sess)\n",
        "\n",
        "train = (train_inp_seqs, train_labs)\n",
        "test = (test_inp_seqs, test_labs)\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "id": "He_tN7Swaivu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df88cf0-d7ca-41c4-e7bd-badbb8ebc80a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lưu dữ liệu train/test"
      ],
      "metadata": {
        "id": "547YB5FAapcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def _save_file(filename, obj):\n",
        "  with open(filename, 'wb') as fn:\n",
        "    pickle.dump(obj, fn)\n",
        "\n",
        "# Tạo folder yoochoose-data-4 để lưu dữ liệu train/test nếu chưa tồn tại\n",
        "if not os.path.exists('dressipi_data_train0.2'):\n",
        "  os.mkdir('dressipi_data_train0.2')\n",
        "\n",
        "# Lưu train/test\n",
        "_save_file('dressipi_data_train0.2/train02.pkl', train)\n",
        "_save_file('dressipi_data_train0.2/test02.pkl', test)"
      ],
      "metadata": {
        "id": "Kra5Au9Maotg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def _load_file(filename):\n",
        "  with open(filename, 'rb') as fn:\n",
        "    data = pickle.load(fn)\n",
        "  return data\n",
        "\n",
        "# Load dữ liệu train/test từ folder\n",
        "train = _load_file('dressipi_data_train0.2/train02.pkl')\n",
        "test = _load_file('dressipi_data_train0.2/test02.pkl')"
      ],
      "metadata": {
        "id": "xaant_vkrnja"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build vocabulary"
      ],
      "metadata": {
        "id": "3saYIMSOsFMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Các token default\n",
        "PAD_token = 0  # token padding cho câu ngắn\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.item2index = {}\n",
        "        self.item2count = {}\n",
        "        self.index2item = {PAD_token: \"PAD\"}\n",
        "        self.num_items = 1  # số lượng mặc định ban đầu là 1 ứng với PAD_token\n",
        "\n",
        "    def addSenquence(self, data):\n",
        "        for sequence in data:\n",
        "          for item in sequence:\n",
        "              self.addItem(item)\n",
        "\n",
        "    # Thêm một item vào hệ thống\n",
        "    def addItem(self, item):\n",
        "        if item not in self.item2index:\n",
        "            self.item2index[item] = self.num_items\n",
        "            self.item2count[item] = 1\n",
        "            self.index2item[self.num_items] = item\n",
        "            self.num_items += 1\n",
        "        else:\n",
        "            self.item2count[item] += 1\n",
        "\n",
        "    # Loại các item dưới ngưỡng xuất hiện min_count\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_items = []\n",
        "\n",
        "        for k, v in self.item2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_items.append(k)\n",
        "\n",
        "        print('keep_items {} / {} = {:.4f}'.format(\n",
        "            len(keep_items), len(self.item2index), len(keep_items) / len(self.item2index)\n",
        "        ))\n",
        "\n",
        "        # Khởi tạo lại từ điển\n",
        "        self.item2index = {}\n",
        "        self.item2count = {}\n",
        "        self.index2item = {PAD_token: \"PAD\"}\n",
        "        self.num_items = 1\n",
        "\n",
        "        # Thêm các items vào từ điển\n",
        "        for item in keep_items:\n",
        "            self.addItem(item)\n",
        "\n",
        "    # Hàm convert sequence về chuỗi các indices\n",
        "    def _seqItem2seqIndex(self, x):\n",
        "        return [voc.item2index[item] if item in voc.item2index else 0 for item in x]"
      ],
      "metadata": {
        "id": "CbHpMcKysGcU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lấy toàn bộ list các itemIds trong các session\n",
        "from itertools import chain\n",
        "seq_targets = [train[1]] + [test[1]]\n",
        "sessionIds = list(chain.from_iterable(seq_targets))\n",
        "sessionIds = set(sessionIds)\n",
        "print('Number of sessionIds: ', len(sessionIds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrqaqhmzsLVF",
        "outputId": "1774734b-3b83-45e5-9f67-31aa44ec05e4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessionIds:  11352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Khởi tạo vocabulary cho bộ dữ liệu\n",
        "voc = Voc('DictItemId')\n",
        "voc.addSenquence(seq_targets)\n",
        "\n",
        "# Convert thử nghiệm một sequence itemIds\n",
        "print('sequence of itemIds: ', train[0][7])\n",
        "print('converted indices: ', voc._seqItem2seqIndex(train[0][7]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abm4-4V2sN1G",
        "outputId": "dfc2f7f7-9f1b-46a6-fa4e-0e4c3035f8e7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence of itemIds:  [2961.0]\n",
            "converted indices:  [6801]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chuyển dữ liệu train, test từ item sang indices của item\n",
        "train_x_index = [voc._seqItem2seqIndex(seq) for seq in train[0]]\n",
        "test_x_index = [voc._seqItem2seqIndex(seq) for seq in test[0]]\n",
        "train_y_index = voc._seqItem2seqIndex(train[1])\n",
        "test_y_index = voc._seqItem2seqIndex(test[1])\n",
        "train_index = (train_x_index, train_y_index)\n",
        "test_index = (test_x_index, test_y_index)"
      ],
      "metadata": {
        "id": "dPPxgFmVsagN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phân chia tập Train/Test/Validation"
      ],
      "metadata": {
        "id": "itMzUJP0s8iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(root='', valid_portion=0.1, maxlen=19, sort_by_len=False, train_set=None, test_set=None):\n",
        "    \"\"\"Load dataset từ root\n",
        "    root: folder dữ liệu train, trong trường hợp train_set, test_set tồn tại thì không sử dụng train_set và test_set\n",
        "    valid_portion: tỷ lệ phân chia dữ liệu validation/train\n",
        "    maxlen: độ dài lớn nhất của sequence\n",
        "    sort_by_len: có sort theo chiều dài các session trước khi chia hay không?\n",
        "    train_set: training dataset\n",
        "    test_set:  test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the dataset\n",
        "    if train_set is None and test_set is None:\n",
        "        path_train_data = os.path.join(root, 'train.pkl')\n",
        "        path_test_data = os.path.join(root, 'test.pkl')\n",
        "        with open(path_train_data, 'rb') as f1:\n",
        "            train_set = pickle.load(f1)\n",
        "\n",
        "        with open(path_test_data, 'rb') as f2:\n",
        "            test_set = pickle.load(f2)\n",
        "\n",
        "    if maxlen:\n",
        "        new_train_set_x = []\n",
        "        new_train_set_y = []\n",
        "        # Lọc dữ liệu sequence đến maxlen\n",
        "        for x, y in zip(train_set[0], train_set[1]):\n",
        "            if len(x) < maxlen:\n",
        "                new_train_set_x.append(x)\n",
        "                new_train_set_y.append(y)\n",
        "            else:\n",
        "                new_train_set_x.append(x[:maxlen])\n",
        "                new_train_set_y.append(y)\n",
        "        train_set = (new_train_set_x, new_train_set_y)\n",
        "        del new_train_set_x, new_train_set_y\n",
        "\n",
        "        new_test_set_x = []\n",
        "        new_test_set_y = []\n",
        "        for xx, yy in zip(test_set[0], test_set[1]):\n",
        "            if len(xx) < maxlen:\n",
        "                new_test_set_x.append(xx)\n",
        "                new_test_set_y.append(yy)\n",
        "            else:\n",
        "                new_test_set_x.append(xx[:maxlen])\n",
        "                new_test_set_y.append(yy)\n",
        "        test_set = (new_test_set_x, new_test_set_y)\n",
        "        del new_test_set_x, new_test_set_y\n",
        "\n",
        "    # phân chia tập train thành train và validation\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    (test_set_x, test_set_y) = test_set\n",
        "\n",
        "    # Trả về indices thứ tự độ dài của mỗi phần tử trong seq\n",
        "    def len_argsort(seq):\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "\n",
        "    # Sắp xếp session theo độ dài tăng dần\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(test_set_x)\n",
        "        test_set_x = [test_set_x[i] for i in sorted_index]\n",
        "        test_set_y = [test_set_y[i] for i in sorted_index]\n",
        "\n",
        "        sorted_index = len_argsort(valid_set_x)\n",
        "        valid_set_x = [valid_set_x[i] for i in sorted_index]\n",
        "        valid_set_y = [valid_set_y[i] for i in sorted_index]\n",
        "\n",
        "    train = (train_set_x, train_set_y)\n",
        "    valid = (valid_set_x, valid_set_y)\n",
        "    test = (test_set_x, test_set_y)\n",
        "    return train, valid, test"
      ],
      "metadata": {
        "id": "vprh-NX3s7jT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "XqAlGwtitLdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class RecSysDataset(Dataset):\n",
        "    \"\"\"define the pytorch Dataset class for yoochoose and diginetica datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        print('-'*50)\n",
        "        print('Dataset info:')\n",
        "        print('Number of sessions: {}'.format(len(data[0])))\n",
        "        print('-'*50)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        session_items = self.data[0][index]\n",
        "        target_item = self.data[1][index]\n",
        "        return session_items, target_item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])"
      ],
      "metadata": {
        "id": "xyb-k3pAtIx0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm phụ trợ\n",
        "import torch\n",
        "\n",
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    Hàm số này sẽ được sử dụng để pad session về max length\n",
        "    Args:\n",
        "      data: batch truyền vào\n",
        "    return:\n",
        "      batch data đã được pad length có shape maxlen x batch_size\n",
        "    \"\"\"\n",
        "    # Sort batch theo độ dài của input_sequence từ cao xuống thấp\n",
        "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    lens = [len(sess) for sess, label in data]\n",
        "    labels = []\n",
        "    # Padding batch size\n",
        "    padded_sesss = torch.zeros(len(data), max(lens)).long()\n",
        "    for i, (sess, label) in enumerate(data):\n",
        "        padded_sesss[i,:lens[i]] = torch.LongTensor(sess)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Transpose dữ liệu từ batch_size x maxlen --> maxlen x batch_size\n",
        "    padded_sesss = padded_sesss.transpose(0,1)\n",
        "    return padded_sesss, torch.tensor(labels).long(), lens"
      ],
      "metadata": {
        "id": "iIDbw8LitOiG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metric"
      ],
      "metadata": {
        "id": "KsbuMl6ptbEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_recall(indices, targets):\n",
        "    \"\"\"\n",
        "    Tính toán chỉ số recall cho một tập hợp predictions và targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices được dự báo từ mô hình model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "    \"\"\"\n",
        "    # copy targets k lần để trở thành kích thước Bxk\n",
        "    targets = targets.view(-1, 1).expand_as(indices)\n",
        "    # so sánh targets với indices để tìm ra vị trí mà khách hàng sẽ hit.\n",
        "    hits = (targets == indices).to(device)\n",
        "    hits = hits.double()\n",
        "    if targets.size(0) == 0:\n",
        "        return 0\n",
        "    # Đếm số hit\n",
        "    n_hits = torch.sum(hits)\n",
        "    recall = n_hits / targets.size(0)\n",
        "    return recall\n",
        "\n",
        "\n",
        "def get_mrr(indices, targets):\n",
        "    \"\"\"\n",
        "    Tính toán chỉ số MRR cho một tập hợp predictions và targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices được dự báo từ mô hình model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the MRR score\n",
        "    \"\"\"\n",
        "    tmp = targets.view(-1, 1)\n",
        "    targets = tmp.expand_as(indices)\n",
        "    hits = (targets == indices).to(device)\n",
        "    hits = hits.double()\n",
        "    if hits.sum() == 0:\n",
        "      return 0\n",
        "    argsort = []\n",
        "    for i in np.arange(hits.shape[0]):\n",
        "      index_col = torch.where(hits[i, :] == 1)[0]+1\n",
        "      if index_col.shape[0] != 0:\n",
        "        argsort.append(index_col.double())\n",
        "    inv_argsort = [1/item for item in argsort]\n",
        "    mrr = sum(inv_argsort)/hits.shape[0]\n",
        "    return mrr\n",
        "\n",
        "\n",
        "def evaluate(logits, targets, k=20):\n",
        "    \"\"\"\n",
        "    Đánh giá model sử dụng Recall@K, MRR@K scores.\n",
        "    Args:\n",
        "        logits (B,C): torch.LongTensor. giá trị predicted logit cho itemId tiếp theo.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "        mrr (float): the mrr score\n",
        "    \"\"\"\n",
        "    # Tìm ra indices của topk lớn nhất các giá trị dự báo.\n",
        "    _, indices = torch.topk(logits, k, -1)\n",
        "    recall = get_recall(indices, targets)\n",
        "    mrr = get_mrr(indices, targets)\n",
        "    return recall, mrr"
      ],
      "metadata": {
        "id": "CE9JLKXMtdU-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "logits = torch.tensor([[0.1, 0.2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)\n",
        "\n",
        "targets = torch.tensor([1, 2, 2]).to(device)\n",
        "\n",
        "evaluate(logits = logits, targets = targets, k = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjaSlyNdtipX",
        "outputId": "656391bc-e0fa-4d44-8ca3-b7703619ddbc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1., dtype=torch.float64), tensor([0.8333], dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model NARM"
      ],
      "metadata": {
        "id": "bcWAlCNUdV_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class NARM(nn.Module):\n",
        "    def __init__(self, hidden_size, n_items, embedding_dim, n_layers=1, dropout=0.25):\n",
        "        super(NARM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_items = n_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(self.n_items, self.embedding_dim, padding_idx = 0)\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        # set bidirectional = True for bidirectional\n",
        "        # https://pytorch.org/docs/stable/nn.html?highlight=gru#torch.nn.GRU to get more information\n",
        "        self.gru = nn.GRU(input_size = hidden_size, # number of expected feature of input x\n",
        "                          hidden_size = hidden_size, # number of expected feature of hidden state\n",
        "                          num_layers = n_layers, # number of GRU layers\n",
        "                          dropout=(0 if n_layers == 1 else dropout), # dropout probability apply in encoder network\n",
        "                          bidirectional=True # one or two directions.\n",
        "                         )\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.n_layers)\n",
        "        self.a_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.a_2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.v_t = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "        self.ct_dropout = nn.Dropout(0.5)\n",
        "        self.b = nn.Linear(self.embedding_dim, 2 * self.hidden_size, bias=False)\n",
        "        self.sf = nn.Softmax()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        \"\"\"\n",
        "        input_seq: Batch input_sequence. Shape: max_len x batch_size\n",
        "        input_lengths: Batch input lengths. Shape: batch_size\n",
        "        \"\"\"\n",
        "        # Step 1: Convert sequence indexes to embeddings\n",
        "        # shape: (max_length , batch_size , hidden_size)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module. Padding zero when length less than max_length of input_lengths.\n",
        "        # shape: (max_length , batch_size , hidden_size)\n",
        "        packed = pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "        # Step 2: Forward packed through GRU\n",
        "        # outputs is output of final GRU layer\n",
        "        # hidden is concatenate of all hidden states corresponding with each time step.\n",
        "        # outputs shape: (max_length , batch_size , hidden_size x num_directions)\n",
        "        # hidden shape: (n_layers x num_directions , batch_size , hidden_size)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding. Revert of pack_padded_sequence\n",
        "        # outputs shape: (max_length , batch_size , hidden_size x num_directions)\n",
        "        outputs, length = pad_packed_sequence(outputs)\n",
        "\n",
        "        # Step 3: Global Encoder & Local Encoder\n",
        "        # num_directions = 1 -->\n",
        "        # outputs shape:(max_length , batch_size , hidden_size)\n",
        "        # hidden shape: (n_layers , batch_size , hidden_size)\n",
        "        # lấy hidden state tại time step cuối cùng\n",
        "        ht = hidden[-1]\n",
        "        # reshape outputs\n",
        "        outputs = outputs.permute(1, 0, 2) # [batch_size, max_length, hidden_size]\n",
        "        c_global = ht\n",
        "        # Flatten outputs thành shape: [batch_size, max_length, hidden_size]\n",
        "        gru_output_flatten = outputs.contiguous().view(-1, self.hidden_size)\n",
        "        # Thực hiện một phép chiếu linear projection để tạo các latent variable có shape [batch_size, max_length, hidden_size]\n",
        "        q1 = self.a_1(gru_output_flatten).view(outputs.size())\n",
        "        # Thực hiện một phép chiếu linear projection để tạo các latent variable có shape [batch_size, max_length, hidden_size]\n",
        "        q2 = self.a_2(ht)\n",
        "        # Ma trận mask đánh dấu vị trí khác 0 trên padding sequence.\n",
        "        mask = torch.where(input_seq.permute(1, 0) > 0, torch.tensor([1.], device = self.device), torch.tensor([0.], device = self.device)) # batch_size x max_len\n",
        "        # Điều chỉnh shape\n",
        "        q2_expand = q2.unsqueeze(1).expand_as(q1) # shape [batch_size, max_len, hidden_size]\n",
        "        q2_masked = mask.unsqueeze(2).expand_as(q1) * q2_expand # batch_size x max_len x hidden_size\n",
        "        # Tính trọng số alpha đo lường similarity giữa các hidden state\n",
        "        alpha = self.v_t(torch.sigmoid(q1 + q2_masked).view(-1, self.hidden_size)).view(mask.size()) # batch_size x max_len\n",
        "        alpha_exp = alpha.unsqueeze(2).expand_as(outputs) # batch_size x max_len x hidden_size\n",
        "        # Tính linear combinition của các hidden state\n",
        "        c_local = torch.sum(alpha_exp * outputs, 1) # (batch_size x hidden_size)\n",
        "\n",
        "        # Véc tơ combinition tổng hợp\n",
        "        c_t = torch.cat([c_local, c_global], 1) # batch_size x (2*hidden_size)\n",
        "        c_t = self.ct_dropout(c_t)\n",
        "        # Tính scores\n",
        "\n",
        "        # Step 4: Decoder\n",
        "        # embedding cho toàn bộ các item\n",
        "        item_indices = torch.arange(self.n_items).to(device) # 1 x n_items\n",
        "        item_embs = self.embedding(item_indices) # n_items x embedding_dim\n",
        "        # reduce dimension by bi-linear projection\n",
        "        B = self.b(item_embs).permute(1, 0) # (2*hidden_size) x n_items\n",
        "        scores = torch.matmul(c_t, B) # batch_size x n_items\n",
        "        # scores = self.sf(scores)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "XsuSpwf6dXLQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kiểm tra model NARM\n",
        "# Thử nghiệm model bằng cách giả lập 1 input và thực hiện quá trình feed forward\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "hidden_size = 3\n",
        "n_layers = 7\n",
        "# embedding = nn.Embedding(11000, hidden_size)\n",
        "input_variable = torch.tensor([[  66,  369,   66, 1272],\n",
        "                                [ 567,  183,   28,  616],\n",
        "                                [ 392, 1558, 1143,  175],\n",
        "                                [ 394,   31,   31, 5558],\n",
        "                                [   0,    0,    0,    0]]).to(device)\n",
        "\n",
        "lengths =  torch.tensor([5, 5, 5, 5]).to(device)\n",
        "print('input_seq: \\n', input_variable)\n",
        "print('input_lengths: \\n', lengths)\n",
        "model_test = NARM(hidden_size = hidden_size, n_items  = 100000, embedding_dim = 100, n_layers=1, dropout=0.25).to(device)\n",
        "print('model phrase: \\n', model_test)\n",
        "scores = model_test.forward(input_seq = input_variable, input_lengths = lengths)\n",
        "print('probability distribution: ', scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiCxOA6Jdbqm",
        "outputId": "669e66e8-6e65-4b1e-c1c0-c76da253765a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_seq: \n",
            " tensor([[  66,  369,   66, 1272],\n",
            "        [ 567,  183,   28,  616],\n",
            "        [ 392, 1558, 1143,  175],\n",
            "        [ 394,   31,   31, 5558],\n",
            "        [   0,    0,    0,    0]])\n",
            "input_lengths: \n",
            " tensor([5, 5, 5, 5])\n",
            "model phrase: \n",
            " NARM(\n",
            "  (embedding): Embedding(100000, 100, padding_idx=0)\n",
            "  (gru): GRU(100, 3)\n",
            "  (emb_dropout): Dropout(p=0.25, inplace=False)\n",
            "  (a_1): Linear(in_features=3, out_features=3, bias=False)\n",
            "  (a_2): Linear(in_features=3, out_features=3, bias=False)\n",
            "  (v_t): Linear(in_features=3, out_features=1, bias=False)\n",
            "  (ct_dropout): Dropout(p=0.5, inplace=False)\n",
            "  (b): Linear(in_features=100, out_features=6, bias=False)\n",
            "  (sf): Softmax(dim=None)\n",
            ")\n",
            "probability distribution:  torch.Size([4, 100000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation"
      ],
      "metadata": {
        "id": "Dxql6QPAtoDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, model):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    mrrs = []\n",
        "    with torch.no_grad():\n",
        "        for seq, target, lens in valid_loader:\n",
        "            seq = seq.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs = model(seq, lens)\n",
        "            logits = F.softmax(outputs, dim = 1)\n",
        "            recall, mrr = evaluate(logits, target, k = args['topk'])\n",
        "            recalls.append(recall)\n",
        "            mrrs.append(mrr)\n",
        "\n",
        "    mean_recall = torch.mean(torch.stack(recalls))\n",
        "    # mean_mrr = torch.mean(torch.stack(mrrs))\n",
        "    return mean_recall #, mean_mrr"
      ],
      "metadata": {
        "id": "qCNFThZftnt6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training model"
      ],
      "metadata": {
        "id": "7ysBrZ0qtu1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "\n",
        "args = {\n",
        "    'dataset_path':'../input/dressipi_recsys2022_dataset/train_sessions.csv',\n",
        "    'batch_size': 256,\n",
        "    'hidden_size': 100,\n",
        "    'embed_dim': 50,\n",
        "    'epoch': 5,\n",
        "    'lr':0.01,\n",
        "    'lr_dc':0.1,\n",
        "    'lr_dc_step':80,\n",
        "    'test':None,\n",
        "    'topk':100,\n",
        "    'valid_portion':0.1\n",
        "}\n",
        "\n",
        "here = os.path.dirname(os.getcwd())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def main():\n",
        "    print('Loading data...')\n",
        "    train_data, valid_data, test_data = load_data(train_set=train_index, test_set=test_index)\n",
        "    train_data = RecSysDataset(train_data)\n",
        "    valid_data = RecSysDataset(valid_data)\n",
        "    test_data = RecSysDataset(test_data)\n",
        "    train_loader = DataLoader(train_data, batch_size = args['batch_size'], shuffle = True, collate_fn = collate_fn)\n",
        "    valid_loader = DataLoader(valid_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "    test_loader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "    print('Complete load data!')\n",
        "    n_items = voc.num_items\n",
        "    model = NARM(hidden_size = args['hidden_size'], n_items = n_items, embedding_dim = args['embed_dim'], n_layers=2, dropout=0.25).to(device)\n",
        "    print('complete load model!')\n",
        "\n",
        "    if args['test'] == 'store_true':\n",
        "        ckpt = torch.load('latest_checkpoint.pth.tar')\n",
        "        model.load_state_dict(ckpt['state_dict'])\n",
        "        recall, mrr = validate(test_loader, model)\n",
        "        print(\"Test: Recall@{}: {:.4f}, MRR@{}: {:.4f}\".format(args['topk'], recall, args['topk'], mrr))\n",
        "        return model\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), args['lr'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = StepLR(optimizer, step_size = args['lr_dc_step'], gamma = args['lr_dc'])\n",
        "\n",
        "    print('start training!')\n",
        "    for epoch in tqdm(range(args['epoch'])):\n",
        "        # train for one epoch\n",
        "        trainForEpoch(train_loader, model, optimizer, epoch, args['epoch'], criterion, log_aggr = 1000)\n",
        "        scheduler.step(epoch = epoch)\n",
        "        recall = validate(valid_loader, model)\n",
        "        print('Epoch {} validation: Recall@{}: {:.4f}\\n'.format(epoch, args['topk'], recall, args['topk']))\n",
        "\n",
        "        # store best loss and save a model checkpoint\n",
        "        ckpt_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(ckpt_dict, 'latest_checkpoint.pth.tar')\n",
        "    return model\n",
        "\n",
        "\n",
        "def trainForEpoch(train_loader, model, optimizer, epoch, num_epochs, criterion, log_aggr=1000):\n",
        "    model.train()\n",
        "\n",
        "    sum_epoch_loss = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (seq, target, lens) in enumerate(train_loader):\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(seq, lens)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        sum_epoch_loss += loss_val\n",
        "\n",
        "        iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "        if i % log_aggr == 0:\n",
        "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f) (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),\n",
        "                  len(seq) / (time.time() - start)))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff7pHgPZtud_",
        "outputId": "2f056f1e-982c-4315-a141-afbb8b8a06da"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 40723\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 4525\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 1779\n",
            "--------------------------------------------------\n",
            "Complete load data!\n",
            "complete load model!\n",
            "start training!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] epoch 1/5  observation 0/160 batch loss: 9.6058 (avg 9.6058) (25.00 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            " 20%|██        | 1/5 [00:20<01:22, 20.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 validation: Recall@100: 0.0277\n",
            "\n",
            "[TRAIN] epoch 2/5  observation 0/160 batch loss: 9.1972 (avg 9.1972) (28.62 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:40<01:00, 20.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 validation: Recall@100: 0.0568\n",
            "\n",
            "[TRAIN] epoch 3/5  observation 0/160 batch loss: 8.9948 (avg 8.9948) (44.70 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [01:01<00:40, 20.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 validation: Recall@100: 0.0587\n",
            "\n",
            "[TRAIN] epoch 4/5  observation 0/160 batch loss: 8.7970 (avg 8.7970) (53.42 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [01:21<00:20, 20.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 validation: Recall@100: 0.0576\n",
            "\n",
            "[TRAIN] epoch 5/5  observation 0/160 batch loss: 8.5046 (avg 8.5046) (44.31 im/s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [01:43<00:00, 20.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 validation: Recall@100: 0.0593\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()\n",
        "# chứa 'latest_checkpoint.pth.tar'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pJIrilqvn0L",
        "outputId": "34e46432-240d-4f4f-a4ac-d7ff25ecd398"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'dressipi_data_train0.2',\n",
              " 'latest_checkpoint.pth.tar',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "PATH = 'latest_checkpoint.pth.tar'\n",
        "model = NARM(hidden_size = args['hidden_size'], n_items = 13353, embedding_dim = args['embed_dim'], n_layers=2, dropout=0.25).to(device)\n",
        "optimizer = optim.Adam(params = model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Ty19p9tXvrQ8",
        "outputId": "d45852b9-81bf-4014-8026-2477dde20f1a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for NARM:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([11353, 50]) from checkpoint, the shape in current model is torch.Size([13353, 50]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-1d3536c4ffda>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2154\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NARM:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([11353, 50]) from checkpoint, the shape in current model is torch.Size([13353, 50])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lựa chọn ngẫu nhiên một session trên test\n",
        "import numpy as np\n",
        "i = np.random.randint(0, len(test_index[0]))\n",
        "x = [test_index[0][i]]\n",
        "y = [test_index[1][i]]\n",
        "print('item indexes sequence input: ', x)\n",
        "print('item index next output: ', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XaCsKXUvsUb",
        "outputId": "7957cbe7-9d09-418d-82b2-618d79894a45"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item indexes sequence input:  [[834]]\n",
            "item index next output:  [1408]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Khởi tạo test_loader để biến đổi dữ liệu session đưa vào mô hình\n",
        "test_data = RecSysDataset([x, y])\n",
        "test_loader = DataLoader(test_data, batch_size = args['batch_size'], shuffle = False, collate_fn = collate_fn)\n",
        "\n",
        "# Step 2: Dự báo các indice tiếp theo mà khách hàng có khả năng click\n",
        "def _preddict(loader, model):\n",
        "    model.eval()\n",
        "    recalls = []\n",
        "    mrrs = []\n",
        "    j = 1\n",
        "    with torch.no_grad():\n",
        "      for seq, target, lens in loader:\n",
        "        seq = seq.to(device)\n",
        "        target = target.to(device)\n",
        "        outputs = model(seq, lens)\n",
        "        logits = F.softmax(outputs, dim = 1)\n",
        "        _, indices = torch.topk(logits, 20, -1)\n",
        "        print('Is next clicked item in top 20 suggestions: ', (target in indices))\n",
        "        print('Top 20 next item indices suggested: ')\n",
        "    return indices\n",
        "\n",
        "_preddict(test_loader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5uHJ7w2vu6-",
        "outputId": "01b4c910-9fb3-49eb-942e-d35cac68e399"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Dataset info:\n",
            "Number of sessions: 1\n",
            "--------------------------------------------------\n",
            "Is next clicked item in top 20 suggestions:  False\n",
            "Top 20 next item indices suggested: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6377,   372,  1074,   691,  3560,  9063,  6265, 11140, 12450, 12327,\n",
              "           949,   241,  8819, 11417, 12382,   263,  5045,  3197, 10068,  2561]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}