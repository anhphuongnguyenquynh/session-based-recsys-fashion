{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "BphMSIlw03lc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/anhphuongnguyenquynh/session-based-recsys-fashion/main/dressipi_recsys2022_datasets.zip'\n",
        "!wget $url\n",
        "!unzip dressipi_recsys2022_datasets.zip\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ngfs_L4v6ID",
        "outputId": "0aa41c75-d2ff-4a60-af01-4fecb06de05e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 03:15:51--  https://raw.githubusercontent.com/anhphuongnguyenquynh/session-based-recsys-fashion/main/dressipi_recsys2022_datasets.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79384785 (76M) [application/zip]\n",
            "Saving to: ‘dressipi_recsys2022_datasets.zip’\n",
            "\n",
            "dressipi_recsys2022 100%[===================>]  75.71M   178MB/s    in 0.4s    \n",
            "\n",
            "2024-05-05 03:15:53 (178 MB/s) - ‘dressipi_recsys2022_datasets.zip’ saved [79384785/79384785]\n",
            "\n",
            "Archive:  dressipi_recsys2022_datasets.zip\n",
            "   creating: dressipi_recsys2022_dataset/\n",
            "  inflating: dressipi_recsys2022_dataset/README.txt  \n",
            "  inflating: dressipi_recsys2022_dataset/candidate_items.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/item_features.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_final_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_final_sessions.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_full_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_full_sessions.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_leaderboard_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/test_leaderboard_sessions.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/train_purchases.csv  \n",
            "  inflating: dressipi_recsys2022_dataset/train_sessions.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sessions = pd.read_csv('dressipi_recsys2022_dataset/train_sessions.csv')\n",
        "train_purchases = pd.read_csv('dressipi_recsys2022_dataset/train_purchases.csv')\n",
        "item_features = pd.read_csv('dressipi_recsys2022_dataset/item_features.csv')\n",
        "candidate_items = pd.read_csv('dressipi_recsys2022_dataset/candidate_items.csv')\n",
        "test_full_sessions = pd.read_csv('dressipi_recsys2022_dataset/test_full_sessions.csv')\n",
        "test_full_purchases = pd.read_csv('dressipi_recsys2022_dataset/test_full_purchases.csv')"
      ],
      "metadata": {
        "id": "D_CkqaTpwENK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to timetsamp"
      ],
      "metadata": {
        "id": "rQs1tnw60_ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rename column"
      ],
      "metadata": {
        "id": "DbUIw2JM1BAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove short sessions <2"
      ],
      "metadata": {
        "id": "iobBx_bwwKbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader"
      ],
      "metadata": {
        "id": "ty4AAO4XV3Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, path, sep=',', session_key='SessionID', item_key='ItemID', time_key='Time', n_sample=-1, itemmap=None, itemstamp=None, time_sort=False):\n",
        "        # Read csv\n",
        "        self.df = pd.read_csv(path, sep=sep, dtype={session_key: int, item_key: int, time_key: float})\n",
        "        self.session_key = session_key\n",
        "        self.item_key = item_key\n",
        "        self.time_key = time_key\n",
        "        self.time_sort = time_sort\n",
        "        if n_sample > 0:\n",
        "            self.df = self.df[:n_sample]\n",
        "\n",
        "        # Add colummn item index to data\n",
        "        self.add_item_indices(itemmap=itemmap)\n",
        "        \"\"\"\n",
        "        Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
        "        clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
        "        \"\"\"\n",
        "        self.df.sort_values([session_key, time_key], inplace=True)\n",
        "        self.click_offsets = self.get_click_offset()\n",
        "        self.session_idx_arr = self.order_session_idx()\n",
        "\n",
        "    def add_item_indices(self, itemmap=None):\n",
        "        \"\"\"\n",
        "        Add item index column named \"item_idx\" to the df\n",
        "        Args:\n",
        "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
        "        \"\"\"\n",
        "        if itemmap is None:\n",
        "            item_ids = self.df[self.item_key].unique()  # type is numpy.ndarray\n",
        "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
        "                                 index=item_ids)\n",
        "            # Build itemmap is a DataFrame that have 2 columns (self.item_key, 'item_idx)\n",
        "            itemmap = pd.DataFrame({self.item_key: item_ids,\n",
        "                                   'item_idx': item2idx[item_ids].values})\n",
        "        self.itemmap = itemmap\n",
        "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
        "\n",
        "    def get_click_offset(self):\n",
        "        \"\"\"\n",
        "        self.df[self.session_key] return a set of session_key\n",
        "        self.df[self.session_key].nunique() return the size of session_key set (int)\n",
        "        self.df.groupby(self.session_key).size() return the size of each session_id\n",
        "        self.df.groupby(self.session_key).size().cumsum() retunn cumulative sum\n",
        "        \"\"\"\n",
        "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
        "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
        "        return offsets\n",
        "\n",
        "    def order_session_idx(self):\n",
        "        if self.time_sort:\n",
        "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
        "            session_idx_arr = np.argsort(sessions_start_time)\n",
        "        else:\n",
        "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
        "        return session_idx_arr\n",
        "\n",
        "    @property\n",
        "    def items(self):\n",
        "        return self.itemmap[self.item_key].unique()"
      ],
      "metadata": {
        "id": "me73JxGzbBM7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset, batch_size=50):\n",
        "        \"\"\"\n",
        "        A class for creating session-parallel mini-batches.\n",
        "\n",
        "        Args:\n",
        "             dataset (SessionDataset): the session dataset to generate the batches from\n",
        "             batch_size (int): size of the batch\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
        "\n",
        "        Yields:\n",
        "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
        "            target (B,): a Variable that stores the target item indices\n",
        "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
        "        \"\"\"\n",
        "        # initializations\n",
        "        df = self.dataset.df\n",
        "        click_offsets = self.dataset.click_offsets\n",
        "        session_idx_arr = self.dataset.session_idx_arr\n",
        "\n",
        "        iters = np.arange(self.batch_size)\n",
        "        maxiter = iters.max()\n",
        "        start = click_offsets[session_idx_arr[iters]]\n",
        "        end = click_offsets[session_idx_arr[iters] + 1]\n",
        "        mask = []  # indicator for the sessions to be terminated\n",
        "        finished = False\n",
        "\n",
        "        while not finished:\n",
        "            minlen = (end - start).min()\n",
        "            # Item indices(for embedding) for clicks where the first sessions start\n",
        "            idx_target = df.item_idx.values[start]\n",
        "\n",
        "            for i in range(minlen - 1):\n",
        "                # Build inputs & targets\n",
        "                idx_input = idx_target\n",
        "                idx_target = df.item_idx.values[start + i + 1]\n",
        "                input = torch.LongTensor(idx_input)\n",
        "                target = torch.LongTensor(idx_target)\n",
        "                yield input, target, mask\n",
        "\n",
        "            # click indices where a particular session meets second-to-last element\n",
        "            start = start + (minlen - 1)\n",
        "            # see if how many sessions should terminate\n",
        "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
        "            for idx in mask:\n",
        "                maxiter += 1\n",
        "                if maxiter >= len(click_offsets) - 1:\n",
        "                    finished = True\n",
        "                    break\n",
        "                # update the next starting/ending point\n",
        "                iters[idx] = maxiter\n",
        "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
        "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
      ],
      "metadata": {
        "id": "rtj5pzmG-oYF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architectures"
      ],
      "metadata": {
        "id": "OstvkSzgV1y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU4REC(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, final_act='tanh',\n",
        "                 dropout_hidden=.5, dropout_input=0, batch_size=50, embedding_dim=-1, use_cuda=False):\n",
        "        super(GRU4REC, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_hidden = dropout_hidden\n",
        "        self.dropout_input = dropout_input\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.use_cuda = use_cuda\n",
        "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "        self.onehot_buffer = self.init_emb()\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.create_final_activation(final_act)\n",
        "        if self.embedding_dim != -1:\n",
        "            self.look_up = nn.Embedding(input_size, self.embedding_dim)\n",
        "            self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
        "        else:\n",
        "            self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def create_final_activation(self, final_act):\n",
        "        if final_act == 'tanh':\n",
        "            self.final_activation = nn.Tanh()\n",
        "        elif final_act == 'relu':\n",
        "            self.final_activation = nn.ReLU()\n",
        "        elif final_act == 'softmax':\n",
        "            self.final_activation = nn.Softmax()\n",
        "        elif final_act == 'softmax_logit':\n",
        "            self.final_activation = nn.LogSoftmax()\n",
        "        elif final_act.startswith('elu-'):\n",
        "            self.final_activation = nn.ELU(alpha=float(final_act.split('-')[1]))\n",
        "        elif final_act.startswith('leaky-'):\n",
        "            self.final_activation = nn.LeakyReLU(negative_slope=float(final_act.split('-')[1]))\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        '''\n",
        "        Args:\n",
        "            input (B,): a batch of item indices from a session-parallel mini-batch.\n",
        "            target (B,): torch.LongTensor of next item indices from a session-parallel mini-batch.\n",
        "\n",
        "        Returns:\n",
        "            logit (B,C): Variable that stores the logits for the next items in the session-parallel mini-batch\n",
        "            hidden: GRU hidden state\n",
        "        '''\n",
        "\n",
        "        if self.embedding_dim == -1:\n",
        "            embedded = self.onehot_encode(input)\n",
        "            if self.training and self.dropout_input > 0: embedded = self.embedding_dropout(embedded)\n",
        "            embedded = embedded.unsqueeze(0)\n",
        "        else:\n",
        "            embedded = input.unsqueeze(0)\n",
        "            embedded = self.look_up(embedded)\n",
        "\n",
        "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
        "        output = output.view(-1, output.size(-1))  #(B,H)\n",
        "        logit = self.final_activation(self.h2o(output))\n",
        "\n",
        "        return logit, hidden\n",
        "\n",
        "    def init_emb(self):\n",
        "        '''\n",
        "        Initialize the one_hot embedding buffer, which will be used for producing the one-hot embeddings efficiently\n",
        "        '''\n",
        "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
        "        onehot_buffer = onehot_buffer.to(self.device)\n",
        "        return onehot_buffer\n",
        "\n",
        "    def onehot_encode(self, input):\n",
        "        \"\"\"\n",
        "        Returns a one-hot vector corresponding to the input\n",
        "        Args:\n",
        "            input (B,): torch.LongTensor of item indices\n",
        "            buffer (B,output_size): buffer that stores the one-hot vector\n",
        "        Returns:\n",
        "            one_hot (B,C): torch.FloatTensor of one-hot vectors\n",
        "        \"\"\"\n",
        "        self.onehot_buffer.zero_()\n",
        "        index = input.view(-1, 1)\n",
        "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
        "        return one_hot\n",
        "\n",
        "    def embedding_dropout(self, input):\n",
        "        p_drop = torch.Tensor(input.size(0), 1).fill_(1 - self.dropout_input)\n",
        "        mask = torch.bernoulli(p_drop).expand_as(input) / (1 - self.dropout_input)\n",
        "        mask = mask.to(self.device)\n",
        "        input = input * mask\n",
        "        return input\n",
        "\n",
        "    def init_hidden(self):\n",
        "        '''\n",
        "        Initialize the hidden state of the GRU\n",
        "        '''\n",
        "        try:\n",
        "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
        "        except:\n",
        "            self.device = 'cpu'\n",
        "            h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)\n",
        "        return h0"
      ],
      "metadata": {
        "id": "KaD_vfZsa3lU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function"
      ],
      "metadata": {
        "id": "LxZwlnk2flP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "p-gfW5v5ffYJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SampledCrossEntropyLoss(nn.Module):\n",
        "    \"\"\" CrossEntropyLoss with n_classes = batch_size = the number of samples in the session-parallel mini-batch \"\"\"\n",
        "    def __init__(self, use_cuda):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "             use_cuda (bool): whether to use cuda or not\n",
        "        \"\"\"\n",
        "        super(SampledCrossEntropyLoss, self).__init__()\n",
        "        self.xe_loss = nn.CrossEntropyLoss()\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "    def forward(self, logit):\n",
        "        batch_size = logit.size(1)\n",
        "        target = Variable(torch.arange(batch_size).long())\n",
        "        if self.use_cuda:\n",
        "            target = target.cuda()\n",
        "\n",
        "        return self.xe_loss(logit, target)"
      ],
      "metadata": {
        "id": "XUlLQ7mvhSNJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPRLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BPRLoss, self).__init__()\n",
        "\n",
        "    def forward(self, logit):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
        "                         The first dimension corresponds to the batches, and the second\n",
        "                         dimension corresponds to sampled number of items to evaluate\n",
        "        \"\"\"\n",
        "        # differences between the item scores\n",
        "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
        "        # final loss\n",
        "        loss = -torch.mean(F.logsigmoid(diff))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "7waZukKahWP_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TOP1Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TOP1Loss, self).__init__()\n",
        "    def forward(self, logit):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
        "                         The first dimension corresponds to the batches, and the second\n",
        "                         dimension corresponds to sampled number of items to evaluate\n",
        "        \"\"\"\n",
        "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
        "        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "6M22czjdhZYP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TOP1_max(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TOP1_max, self).__init__()\n",
        "\n",
        "    def forward(self, logit):\n",
        "        logit_softmax = F.softmax(logit, dim=1)\n",
        "        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
        "        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "9rqSONNQhbAc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPR_max(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BPR_max, self).__init__()\n",
        "    def forward(self, logit):\n",
        "        logit_softmax = F.softmax(logit, dim=1)\n",
        "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
        "        loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "6kFILRZAhX4r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction(nn.Module):\n",
        "  def __init__(self, loss_type = 'TOP1', use_cuda = False):\n",
        "    ##Abstract loss function that can support many loss functions\n",
        "    super(LossFunction, self).__init__()\n",
        "    self.loss_type = loss_type\n",
        "    self.use_cuda = use_cuda\n",
        "    if loss_type == 'CrossEntropy':\n",
        "      self._loss_fn = SampledCrossEntropyLoss(use_cuda)\n",
        "    elif loss_type == 'TOP1':\n",
        "      self._loss_fn = TOP1Loss()\n",
        "    elif loss_type == 'BPR':\n",
        "      self._loss_fn = BPRLoss()\n",
        "    elif loss_type == 'TOP1_max':\n",
        "      self._loss_fn = TOP1Loss()\n",
        "    elif loss_type == 'BPR_max':\n",
        "      self._loss_fn = BPRLoss()\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def formward(self, logit):\n",
        "    return self._loss_fn(logit)\n",
        "\n"
      ],
      "metadata": {
        "id": "nSimI4B1fvbb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer"
      ],
      "metadata": {
        "id": "CI2OY2BIebz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class Optimizer:\n",
        "  def __init__(self, params, optimizer_type = 'Adagrad', lr = .05,\n",
        "               momentum = 0, weight_decay = 0, eps = 1e-6):\n",
        "    \"\"\"\n",
        "    Optimizer class for handling various kinds of optimizers.\n",
        "    Usage is exactly the same as an instance of torch.optim\n",
        "\n",
        "    Args:\n",
        "      params: torch.nn.Parameter. The NN parameters to optimize\n",
        "      optimizer_type: type of the optimizer to use\n",
        "      lr: learning rate\n",
        "      momentum: momentum, if needed\n",
        "      weight_decay: weight decay, if needed. Equivalent to L2 regularization.\n",
        "      eps: eps parameter, if needed\n",
        "    \"\"\"\n",
        "\n",
        "    if optimizer_type == 'RMSProp':\n",
        "            self.optimizer = optim.RMSprop(params, lr=lr, eps=eps, weight_decay=weight_decay, momentum=momentum)\n",
        "    elif optimizer_type == 'Adagrad':\n",
        "            self.optimizer = optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
        "    elif optimizer_type == 'Adadelta':\n",
        "            self.optimizer = optim.Adadelta(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
        "    elif optimizer_type == 'Adam':\n",
        "            self.optimizer = optim.Adam(params, lr=lr, eps=eps, weight_decay=weight_decay)\n",
        "    elif optimizer_type == 'SparseAdam':\n",
        "            self.optimizer = optim.SparseAdam(params, lr=lr, eps=eps)\n",
        "    elif optimizer_type == 'SGD':\n",
        "            self.optimizer = optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def zero_grad(self):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "      self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "to-xbO5IefFP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metric"
      ],
      "metadata": {
        "id": "ogLj_y-yhi5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get_mrr metric\n",
        "def get_mrr(indices, targets): #Mean Receiprocal Rank --> Average of rank of next item in the session.\n",
        "    \"\"\"\n",
        "    Calculates the MRR score for the given predictions and targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        mrr (float): the mrr score\n",
        "    \"\"\"\n",
        "    tmp = targets.view(-1, 1)\n",
        "    targets = tmp.expand_as(indices)\n",
        "    hits = (targets == indices).nonzero()\n",
        "    ranks = hits[:, -1] + 1\n",
        "    ranks = ranks.float()\n",
        "    rranks = torch.reciprocal(ranks)\n",
        "    mrr = torch.sum(rranks).data / targets.size(0)\n",
        "    return mrr"
      ],
      "metadata": {
        "id": "tP0Sydmghj80"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_recall metric\n",
        "def get_recall(indices, targets): #recall --> wether next item in session is within top K=20 recommended items or not\n",
        "    \"\"\"\n",
        "    Calculates the recall score for the given predictions and targets\n",
        "    Args:\n",
        "        indices (Bxk): torch.LongTensor. top-k indices predicted by the model.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        recall (float): the recall score\n",
        "    \"\"\"\n",
        "    targets = targets.view(-1, 1).expand_as(indices)\n",
        "    hits = (targets == indices).nonzero()\n",
        "    if len(hits) == 0:\n",
        "        return 0\n",
        "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
        "    recall = float(n_hits) / targets.size(0)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "RGF3zFn8hraZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation function\n",
        "def evaluate(indices, targets, k = 10):\n",
        "  \"\"\"\n",
        "  Evaluates the model using metric scores: recall/ mrr/ hit rate\n",
        "\n",
        "  Args:\n",
        "    logits (B,C): torch.LongTensor. The predicted logit for the next items.\n",
        "    targets (B): torch.LongTensor. actual target indices\n",
        "\n",
        "  Returns:\n",
        "    recall (float): the recall score\n",
        "    mrr (float): the mrr score\n",
        "    hitrate (float): the hit rate score\n",
        "  \"\"\"\n",
        "  _, indices = torch.topk(indices, k, -1)\n",
        "  recall = get_recall(indices, targets)\n",
        "  mrr = get_mrr(indices, targets)\n",
        "  return recall, mrr"
      ],
      "metadata": {
        "id": "5PHMHnLriSNk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "r7AjqClrV7Cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wvSF9bdCVsrT"
      },
      "outputs": [],
      "source": [
        "class Evaluation(object):\n",
        "  def __init__(self, model, loss_func, use_cude, k=10):\n",
        "    self.model = model\n",
        "    self.loss_func = loss_func\n",
        "    self.topk = k\n",
        "    self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "  def eval(self, eval_data, batch_size):\n",
        "    self.model.eval() #set model in evaluation mode. normalize layers using running statistics de-activate dropout.\n",
        "    losses = []\n",
        "    recalls = []\n",
        "    mrrs = []\n",
        "    dataloader = Dataloader(eval_data, batch_size) #Check bo lib\n",
        "    with torch.no_grad(): #no backprobagation\n",
        "      hidden = self.model.init_hidden()\n",
        "      for ii, (input, target, mask) in tqdm(enumerate(dataloader), total = len(dataloader.dataset.df) //dataloader.batch_size, miniters = 1000):\n",
        "      #for input, target, mask in dataloader:\n",
        "        input = input.to(self.device)\n",
        "        target = target.to(self.device)\n",
        "        logit, hidden = self.model(input, hidden)\n",
        "        logit_sampled = logit[:, target.view(-1)] #view(-1) to flatten the tensor\n",
        "        loss = self.loss_func(logit_sampled)\n",
        "        recall, mrr = evaluate(logit, target, k = self.topk) #check bo lib\n",
        "\n",
        "        #torch.Tensor.item() to get a Python number from a tensor containing\n",
        "        losses.append(loss.item())\n",
        "        recalls.append(recall)\n",
        "        mrrs.append(mrr)\n",
        "    mean_losses = np.mean(losses)\n",
        "    mean_recall = np.mean(recalls)\n",
        "    mean_mrr = np.mean(mrrs)\n",
        "\n",
        "    return mean_losses, mean_recall, mean_mrr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "6QuiWIijV5br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7E3NoIY7lTbx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "  def __init__(self, model, train_data, eval_data, optim, use_cuda, loss_func, batch_size, args):\n",
        "    self.model = model\n",
        "    self.train_data = train_data\n",
        "    self.eval_data = eval_data\n",
        "    self.optim = optim\n",
        "    self.loss_func = loss_func\n",
        "    self.evalution = evaluation(self.model, self.loss_func, use_cuda, k=args.k_eval)\n",
        "    self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "    self.batch_size = batch_size\n",
        "    self.args = args\n",
        "\n",
        "  def train(self, start_epoch, end_epoch, start_time = None):\n",
        "    if start_time is None:\n",
        "      self.start_time = time.time()\n",
        "    else:\n",
        "      self.start_time = start_time\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch + 1):\n",
        "      st = time.time()\n",
        "      print('Start Epoch #', epoch)\n",
        "      train_loss = self.train_epoch(epoch)\n",
        "      loss, recall, mrr = self.evaluation.eval(self.eval_data, self.batch_size)\n",
        "\n",
        "      print(\"Epoch: {}, train loss: {:.4f}, loss: {:.4f}, recall: {:.4f}, mrr: {:.4f}, time: {}\".format(epoch, train_loss, loss, recall, mrr, time.time() - st))\n",
        "      checkpoint = {\n",
        "          'model': self.model,\n",
        "          'args': self.args,\n",
        "          'epoch': epoch,\n",
        "          'optim': self.optim,\n",
        "          'loss': loss,\n",
        "          'recall': recall,\n",
        "          'mrr': mrr\n",
        "      }\n",
        "      model_name = os.path.join(self.args.checkpoint_dir, \"model_{0:05d}.pt\".format(epoch))\n",
        "      torch.save(checkpoint, model_name)\n",
        "      print('Save model as %s' %model_name)\n",
        "\n",
        "  def train_epoch(self, epoch):\n",
        "    self.model.train()\n",
        "    losses = []\n",
        "\n",
        "    def reset_hidden(hidden, mask):\n",
        "      #check helper function that resets hidden state when some sessions terminate\n",
        "      if len(mask) != 0:\n",
        "        hidden[:, mask,:] = 0\n",
        "        return hidden\n",
        "\n",
        "    hidden = self.model.init_hidden()\n",
        "    dataloader = DataLoader(self.train_data, self.batch_size)\n",
        "\n",
        "    for ii, (input, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
        "      input = input.to(self.device)\n",
        "      target = target.to(self.device)\n",
        "      self.optim.zero_grad()\n",
        "      hidden = reset_hidden(hidden, mask).detach() #.detach() Returns a new Tensor, detached from the current graph.\n",
        "      logit, hidden = self.model(input, hidden)\n",
        "      #output sampling\n",
        "      logit_sampled = logit[:, target.view(-1)]\n",
        "      loss = self.loss_func(logit_sampled)\n",
        "      loss.backward()\n",
        "      self.optim.step()\n",
        "\n",
        "    mean_losses = np.mean(losses)\n",
        "    return mean_losses"
      ],
      "metadata": {
        "id": "ZmQ03HsSlYIR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'hidden_size': 100,\n",
        "    'num_layers': 3,\n",
        "    'batch_size': 50,\n",
        "    'dropout_input': 0,\n",
        "    'dropout_hidden': 0.5,\n",
        "    'n_epochs': 5,\n",
        "    'k_eval': 20,\n",
        "    ###optimizer\n",
        "    'optimizer_type': 'Adagrad',\n",
        "    'final_act': 'tanh',\n",
        "    'lr': 0.01,\n",
        "    'weight_decay': 0,\n",
        "    'momentum': 0,\n",
        "    'eps': 1e-6,\n",
        "    'seed': 22,\n",
        "    'sigma': None,\n",
        "    'embedding_dim': -1,\n",
        "    ##loss function\n",
        "    'loss_type': 'BPR'\n",
        "    'time_sort': False,\n",
        "    'model_name': 'GRU4REC-BPR'\n",
        "    'save_dir': 'checkpoints/'\n",
        "    'data_folder': '../dataset/dataset after/',\n",
        "    #'train_data': '../train.txt',\n",
        "    #'valid_data:: '../valid.txt',\n",
        "    'is_eval': False,\n",
        "    'load_model': None,\n",
        "    'checkpoint_dir': 'checkpoints/',\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "DOFtJf9sJeNI",
        "outputId": "4a7dd3c8-ba98-440d-b876-feec25762cd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-a7b90d4635fb>, line 21)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a7b90d4635fb>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    'time_sort': False,\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n_items\n"
      ],
      "metadata": {
        "id": "A6h8uIiLRWl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  #Load train data\n",
        "\n",
        "  #Load valid data\n",
        "\n",
        "  #Set the parameters\n",
        "\n",
        "  #Training\n",
        "  if args['is_eval'] is False:\n",
        "    ##Initialize the model\n",
        "    model = GRU4REC(input_size, hidden_size, output_size, num_layers=1, final_act='tanh',\n",
        "                 dropout_hidden=.5, dropout_input=0, batch_size=50, embedding_dim=-1, use_cuda=False)\n",
        "    ##Weights initialization\n",
        "    init_model(model)\n",
        "    ##Optimizer\n",
        "    optimizer = Optimizer(model.parameters(), optimizer_type = 'Adagrad', lr = .05,\n",
        "               momentum = 0, weight_decay = 0, eps = 1e-6)\n",
        "    ##Trainer class\n",
        "    trainer = Trainer(model, train_data = args['train_data'], eval_data, optim, use_cuda, loss_func, batch_size, args)\n",
        "    print('###START TRAINING...')\n",
        "    trainer.train(0, n-epochs -1)\n",
        "\n",
        "  #Testing\n",
        "  else:\n",
        "    if args['load_model'] is not None:\n",
        "      print(\"Loading pre-trained model from {}\".format(args['load_model']))\n",
        "      try:\n",
        "        checkpoint = torch.load(args['load_model'])\n",
        "      except:\n",
        "        checkpoint = torch.load(args['load_model'], map_location=lambda storage, loc: storage)\n",
        "      model = checkpoint[\"model\"]\n",
        "      model.gru.flatten_parameters() #reset parameters data pointer so that they can use faster code paths\n",
        "      evaluation = Evaluation(model, loss_function, use_cuda = args['use_cuda'], k = args['k_eval'])\n",
        "      loss, recall, mrr = evaluation.eval(valid_data, batch_size)\n",
        "      print(\"Final result: recall = {:.2f}, mrr = {:.2f}\" .format(recall, mrr))\n",
        "    else:\n",
        "      print(\"No pretrained model was found!\")"
      ],
      "metadata": {
        "id": "3nJopF2RUX_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##train_model = main()"
      ],
      "metadata": {
        "id": "pN6_R0IsK1Ao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}